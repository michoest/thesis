{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Application: Training agents with dynamic restrictions in an obstacle avoidance scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Setup and definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(f'{os.getcwd()}/../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from drama.utils import projection_violation_fn\n",
    "from drama.wrapper import RestrictionWrapper\n",
    "\n",
    "from agent import evaluate, TD3\n",
    "from env import NavigationEnvironment\n",
    "from restrictor import NavigationRestrictor\n",
    "from utils import render\n",
    "from examples.utils import ReplayBuffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({\n",
    "    \"pgf.texsystem\": \"pdflatex\",\n",
    "    'font.family': 'serif',\n",
    "    'legend.fontsize': 3,\n",
    "    'text.usetex': True,\n",
    "    \"pgf.rcfonts\": False\n",
    "})\n",
    "\n",
    "sns.set_theme()\n",
    "sns.set_style(\"ticks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "experiment_name = 'Projection'\n",
    "restriction_violation_fns = projection_violation_fn # Choose from \"do_nothing_on_invalid\" and \"projection\"\n",
    "seed = 49                                           # We tested with seeds 46, 47, 48, 49\n",
    "total_training_timesteps = 50000\n",
    "evaluation_frequency = 500\n",
    "evaluation_envs = 20                                # Number of unique evaluation environments\n",
    "sample_from_restricted_space = False\n",
    "\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = 'results/metrics.pkl'\n",
    "\n",
    "if path:\n",
    "    with open(path, 'rb') as f:\n",
    "        results_dataframe = pickle.load(f)\n",
    "else:\n",
    "    results_dataframe = pd.DataFrame(columns=['seed', 'return', 'solved', 'steps', 'training_steps', 'name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env_config = {\n",
    "    'HEIGHT': 15.0,\n",
    "    'WIDTH': 15.0,\n",
    "    'STEPS_PER_EPISODE': 60,\n",
    "    'ACTION_RANGE': 220,\n",
    "    'DT': 1.0,\n",
    "    'TIMESTEP_PENALTY_COEFFICIENT': 0.05,\n",
    "    'REWARD_COLLISION': -1.0,\n",
    "    'REWARD_GOAL': 5.0,\n",
    "    'REWARD_COEFFICIENT': 10.0,\n",
    "    'AGENT_RADIUS': 0.5,\n",
    "    'AGENT_PERSPECTIVE': 90,\n",
    "    'AGENT_STEP_SIZE': 1.0,\n",
    "    'AGENT_X': 1.5,\n",
    "    'AGENT_Y': 1.5,\n",
    "    'GOAL_RADIUS': 1.0,\n",
    "    'GOAL_X': 12.0,\n",
    "    'GOAL_y': 12.0\n",
    "}\n",
    "environment = NavigationEnvironment(env_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "restrictor = NavigationRestrictor(obstacle_count=7,\n",
    "                                  obstacle_position_covariance=[[4.0, 0.0], [0.0, 4.0]],\n",
    "                                  obstacle_mean_size=1.0,\n",
    "                                  obstacle_variance_size=0.2,\n",
    "                                  obstacle_size_range=0.5,\n",
    "                                  start_seed=50,\n",
    "                                  safety_angle=8,\n",
    "                                  min_angle=-110.0,\n",
    "                                  max_angle=110.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "restricted_environment = RestrictionWrapper(environment, restrictor,\n",
    "                                            restriction_violation_fns=restriction_violation_fns,\n",
    "                                            return_object=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "td3_config = {\n",
    "    'state_dim': 6,\n",
    "    'action_dim': 1,\n",
    "    'max_action': 110.0,\n",
    "    'discount': 0.99,\n",
    "    'tau': 0.005,\n",
    "    'policy_noise': 0.2,\n",
    "    'noise_clip:': 0.5,\n",
    "    'policy_freq': 2,\n",
    "    'exploration_noise': 0.2,\n",
    "    'exploration_noise_final': 0.02,\n",
    "    'exploration_timesteps': 30000,\n",
    "    'batch_size': 256,\n",
    "    'train_after_timesteps': 2000,\n",
    "    'learning_rate_actor': 1e-5,\n",
    "    'learning_rate_critic': 1e-5\n",
    "}\n",
    "\n",
    "td3 = TD3(**td3_config)\n",
    "replay_buffer = ReplayBuffer(state_dim=td3_config['state_dim'], action_dim=td3_config['action_dim'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize training parameters\n",
    "evaluation_metrics = {\n",
    "    'average_return': [],\n",
    "    'average_episodes_solved': [],\n",
    "    'average_steps': []\n",
    "}\n",
    "episode_num = 0\n",
    "training_timesteps = 0\n",
    "exploration_annealing_rate = (td3_config['exploration_noise'] - td3_config['exploration_noise_final']\n",
    "                              ) / td3_config['exploration_timesteps']\n",
    "noise_factor = td3_config['exploration_noise']\n",
    "\n",
    "# Initial evaluation\n",
    "ev_reward, ev_solved, ev_steps = evaluate(td3, env_config, restriction_violation_fns, evaluation_envs)\n",
    "evaluation_metrics['average_return'].append(ev_reward)\n",
    "evaluation_metrics['average_episodes_solved'].append(ev_solved)\n",
    "evaluation_metrics['average_steps'].append(ev_steps)\n",
    "\n",
    "# Training loop\n",
    "pbar = tqdm(total=total_training_timesteps)\n",
    "while training_timesteps < total_training_timesteps:\n",
    "    restricted_environment.reset()\n",
    "    episode_reward = 0\n",
    "    episode_timesteps = 0\n",
    "    episode_num += 1\n",
    "    observation = None\n",
    "    action = None\n",
    "    last_td3_action = None\n",
    "\n",
    "    for agent in restricted_environment.agent_iter():\n",
    "        next_observation, reward, termination, truncation, info = restricted_environment.last()\n",
    "\n",
    "        # Turn of the agent\n",
    "        if agent == 'agent_0':\n",
    "            episode_reward += reward\n",
    "\n",
    "            flattened_next_observation = next_observation['observation']\n",
    "\n",
    "            if episode_timesteps > 0:\n",
    "                replay_buffer.add(observation,\n",
    "                                  last_td3_action,\n",
    "                                  flattened_next_observation,\n",
    "                                  reward,\n",
    "                                  termination or truncation)\n",
    "            observation = flattened_next_observation\n",
    "\n",
    "            training_timesteps += 1\n",
    "            pbar.update(1)\n",
    "            if training_timesteps < td3_config['train_after_timesteps']:\n",
    "                if sample_from_restricted_space:\n",
    "                    action = next_observation['restriction'].sample()\n",
    "                else:\n",
    "                    action = np.random.uniform(-110.0, 110.0, (1,))\n",
    "            else:\n",
    "                det_action = td3.select_action(observation)\n",
    "                noise = np.random.normal(0, td3_config['max_action'] * noise_factor,\n",
    "                                         size=td3_config['action_dim'])\n",
    "                action = (det_action + noise).clip(-td3_config['max_action'], td3_config['max_action'])\n",
    "\n",
    "                noise_factor = max([noise_factor - exploration_annealing_rate,\n",
    "                                    td3_config['exploration_noise_final']])\n",
    "\n",
    "            if training_timesteps >= td3_config['train_after_timesteps']:\n",
    "                td3.train(replay_buffer, td3_config['batch_size'])\n",
    "            last_td3_action = action\n",
    "        # Or restrictor\n",
    "        else:\n",
    "            action = restrictor.act(next_observation)\n",
    "\n",
    "        # None action if episode is done\n",
    "        if termination or truncation:\n",
    "            action = None\n",
    "        elif agent == 'agent_0':\n",
    "            episode_timesteps += 1\n",
    "\n",
    "        restricted_environment.step(action)\n",
    "        if agent == 'agent_0' and training_timesteps % evaluation_frequency == 0:\n",
    "            ev_reward, ev_solved, ev_steps = evaluate(td3, env_config, restriction_violation_fns, evaluation_envs)\n",
    "            evaluation_metrics['average_return'].append(ev_reward)\n",
    "            evaluation_metrics['average_episodes_solved'].append(ev_solved)\n",
    "            evaluation_metrics['average_steps'].append(ev_steps)\n",
    "\n",
    "        if training_timesteps >= total_training_timesteps:\n",
    "            break\n",
    "\n",
    "# Save results in dataframe\n",
    "results_dataframe = pd.concat([results_dataframe, pd.DataFrame({\n",
    "    'seed': [seed] * len(evaluation_metrics['average_return']),\n",
    "    'return': evaluation_metrics['average_return'],\n",
    "    'solved': evaluation_metrics['average_episodes_solved'],\n",
    "    'steps': evaluation_metrics['average_steps'],\n",
    "    'training_steps': range(0, total_training_timesteps+evaluation_frequency, evaluation_frequency),\n",
    "    'name': [experiment_name] * len(evaluation_metrics['average_return'])\n",
    "})]).reset_index(drop=True)\n",
    "\n",
    "pbar.close()\n",
    "restricted_environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "td3.save('results/models/projection_49')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results_dataframe.to_pickle('results/metrics.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "td3.load('results/models/do_nothing_49')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6.5, 4.5))\n",
    "ax = sns.lineplot(data=results_dataframe, x='training_steps', y='return', hue='name', errorbar='sd')\n",
    "ax.legend(title=None, loc='lower right', facecolor='white')\n",
    "plt.ylim(-40.0, 140.0)\n",
    "plt.xlim(0.0, 50000.0)\n",
    "plt.ylabel('Return')\n",
    "plt.xlabel('Time Steps')\n",
    "ax.get_xaxis().set_major_formatter(\n",
    "    matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "fig.tight_layout()\n",
    "plt.savefig('results/return-navigations.pdf', format='pdf', bbox_inches='tight', pad_inches = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 4))\n",
    "ax = sns.lineplot(data=results_dataframe, x='training_steps', y='solved', hue='name', errorbar='sd')\n",
    "ax.legend(title=None, loc='center right', facecolor='white')\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.xlim(0.0, 50000.0)\n",
    "plt.ylabel('Solved Episodes')\n",
    "plt.xlabel('Time Steps')\n",
    "ax.get_xaxis().set_major_formatter(\n",
    "    matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "ax.yaxis.set_major_formatter(matplotlib.ticker.PercentFormatter(1.0))\n",
    "fig.tight_layout()\n",
    "plt.savefig('results/solved-navigations.pdf', format='pdf', bbox_inches='tight', pad_inches = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6.5, 4.5))\n",
    "ax = sns.lineplot(data=results_dataframe, x='training_steps', y='steps', hue='name', errorbar='sd')\n",
    "ax.legend(title=None, loc='center right', facecolor='white')\n",
    "plt.ylim(0.0, 62.0)\n",
    "plt.xlim(0.0, 50000.0)\n",
    "plt.ylabel('Steps')\n",
    "plt.xlabel('Time Steps')\n",
    "ax.get_xaxis().set_major_formatter(\n",
    "    matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "fig.tight_layout()\n",
    "plt.savefig('results/steps-navigations.pdf', format='pdf', bbox_inches='tight', pad_inches = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "render(td3, env_config, restriction_violation_fns, seed=12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
