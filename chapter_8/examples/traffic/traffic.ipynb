{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application: Learning optimal restrictions in a continuous-action game"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(f'{os.getcwd()}/../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "from gymnasium.spaces import Discrete, Box, Space\n",
    "\n",
    "from drama.restrictions import DiscreteVectorRestriction\n",
    "from drama.wrapper import RestrictionWrapper\n",
    "from drama.restrictors import Restrictor, RestrictorActionSpace, DiscreteVectorActionSpace\n",
    "from drama.utils import flatdim, flatten, unflatten\n",
    "\n",
    "from examples.utils import play, ReplayBuffer\n",
    "from examples.traffic.env import TrafficEnvironment\n",
    "from examples.traffic.agent import TrafficAgent\n",
    "from examples.traffic.restrictor import TrafficRestrictor\n",
    "\n",
    "from examples.traffic.utils import create_graph, analyze_graph, edge_path_to_node_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = create_graph([\n",
    "    ((0, 1), (0, 8, 1)), \n",
    "    ((0, 2), (11, 0, 0)), \n",
    "    ((1, 2), (1, 0, 0)), \n",
    "    ((1, 3), (11, 0, 0)), \n",
    "    ((2, 3), (0, 8, 1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_agent_routes = [(0, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def powerset(iterable):\n",
    "    s = list(iterable)\n",
    "    return itertools.chain.from_iterable(itertools.combinations(s, r) for r in range(len(s)+1))\n",
    "\n",
    "number_of_nodes = graph.number_of_nodes()\n",
    "number_of_edges = graph.number_of_edges()\n",
    "\n",
    "edge_list = list(graph.edges)\n",
    "edge_indices = {e: i for i, e in enumerate(edge_list)}\n",
    "edge_latencies = {i: graph[s][t][\"latency\"] for i, [s, t] in enumerate(edge_list)}\n",
    "\n",
    "minimum_node_set = set(sum(possible_agent_routes, tuple()))\n",
    "\n",
    "valid_edge_restrictions = []\n",
    "for allowed_edges in powerset(range(number_of_edges)):\n",
    "    subgraph = graph.edge_subgraph(edge_list[i] for i in allowed_edges)\n",
    "    if minimum_node_set.issubset(subgraph.nodes) and all(nx.has_path(subgraph, s, t) for s, t in possible_agent_routes):\n",
    "        valid_edge_restrictions.append(set(allowed_edges))\n",
    "\n",
    "route_list = [tuple(edge_indices[e] for e in path)\n",
    "        for s, t in possible_agent_routes\n",
    "        for path in nx.all_simple_edge_paths(graph, s, t)\n",
    "]\n",
    "number_of_routes = len(route_list)\n",
    "\n",
    "routes = {\n",
    "        (s, t): [\n",
    "            tuple(edge_indices[e] for e in path)\n",
    "            for path in nx.all_simple_edge_paths(graph, s, t)\n",
    "        ]\n",
    "        for s in range(number_of_nodes)\n",
    "        for t in range(number_of_nodes)\n",
    "        if s != t\n",
    "    }\n",
    "route_indices = {tuple(r): i for i, r in enumerate(route_list)}\n",
    "\n",
    "valid_route_restrictions = [np.array([set(route).issubset(edge_restriction) for route in route_list]) for edge_restriction in valid_edge_restrictions]\n",
    "\n",
    "source_target_map = [(s, t) for s, t in possible_agent_routes for _ in nx.all_simple_edge_paths(graph, s, t)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: Without Governance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'possible_routes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m number_of_agents \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      3\u001b[0m agents \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124magent_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m: TrafficAgent(routes, source_target_map) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m (number_of_agents)}\n\u001b[0;32m----> 4\u001b[0m env \u001b[38;5;241m=\u001b[39m TrafficEnvironment(graph, \u001b[38;5;28mlist\u001b[39m(agents), \u001b[43mpossible_routes\u001b[49m, number_of_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      5\u001b[0m policies \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mid\u001b[39m: agent\u001b[38;5;241m.\u001b[39mact \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m, agent \u001b[38;5;129;01min\u001b[39;00m agents\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m      7\u001b[0m trajectory \u001b[38;5;241m=\u001b[39m play(env, policies, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, record_trajectory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'possible_routes' is not defined"
     ]
    }
   ],
   "source": [
    "number_of_agents = 2\n",
    "\n",
    "agents = {f'agent_{i}': TrafficAgent(routes, source_target_map) for i in range (number_of_agents)}\n",
    "env = TrafficEnvironment(graph, list(agents), possible_routes, number_of_steps=100)\n",
    "policies = {id: agent.act for id, agent in agents.items()}\n",
    "\n",
    "trajectory = play(env, policies, max_iter=50, verbose=False, record_trajectory=True, render_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory.groupby('agent')['reward'].plot(legend=True, xlabel='Time step', ylabel='Reward');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax, *_ = trajectory.groupby('agent')['action'].plot(style='.', legend=True)\n",
    "ax.set_yticks(list(route_indices.values()), [edge_path_to_node_path(route, edge_list) for route in route_indices.keys()]);\n",
    "ax.set_ylabel('Route taken')\n",
    "ax.set_xlabel('Time step')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With governance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "analyze_graph() missing 1 required positional argument: 'possible_agent_routes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m number_of_agents \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m----> 3\u001b[0m edge_list, edge_indices, edge_latencies, routes, route_list, route_indices \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m number_of_edges \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mnumber_of_edges()\n\u001b[1;32m      6\u001b[0m agents \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124magent_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m: TrafficAgent(routes, route_indices, edge_indices) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m (number_of_agents)}\n",
      "\u001b[0;31mTypeError\u001b[0m: analyze_graph() missing 1 required positional argument: 'possible_agent_routes'"
     ]
    }
   ],
   "source": [
    "number_of_agents = 2\n",
    "\n",
    "edge_list, edge_indices, edge_latencies, routes, route_list, route_indices = analyze_graph(graph)\n",
    "number_of_edges = graph.number_of_edges()\n",
    "\n",
    "agents = {f'agent_{i}': TrafficAgent(routes, route_indices, edge_indices) for i in range (number_of_agents)}\n",
    "env = TrafficEnvironment(graph, list(agents), possible_routes, number_of_steps=100)\n",
    "\n",
    "restrictor = TrafficRestrictor(Box(0, np.inf, shape=(number_of_edges, )), DiscreteVectorActionSpace(Discrete(len(routes))))\n",
    "wrapper = RestrictionWrapper(env, restrictor, restrictor_reward_fns={'restrictor_0': lambda env, rewards: rewards[env.agent_selection]})\n",
    "\n",
    "policies = {**{id: agent.act for id, agent in agents.items()}, 'restrictor_0': restrictor.act}\n",
    "\n",
    "trajectory = play(wrapper, policies, max_iter=50, verbose=False, record_trajectory=True, render_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory.groupby('agent')['reward'].plot(legend=True, xlabel='Time step', ylabel='Reward');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax, *_ = trajectory[trajectory['agent'] != 'restrictor_0'].groupby('agent')['action'].plot(style='.', legend=True)\n",
    "ax.set_yticks(list(route_indices.values()), [edge_path_to_node_path(route, edge_list) for route in route_indices.keys()]);\n",
    "ax.set_ylabel('Route taken')\n",
    "ax.set_xlabel('Time step')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With self-learning restrictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_agents = 2\n",
    "\n",
    "total_timesteps = 500_000\n",
    "\n",
    "number_of_edges = graph.number_of_edges()\n",
    "\n",
    "agents = {f'agent_{i}': TrafficAgent(route_list, source_target_map) for i in range (number_of_agents)}\n",
    "restrictor = TrafficRestrictor(number_of_edges, number_of_routes,\n",
    "                               valid_route_restrictions, total_timesteps=total_timesteps)\n",
    "\n",
    "env = TrafficEnvironment(graph, list(agents), possible_agent_routes, number_of_routes, edge_latencies, route_list, number_of_steps=100)\n",
    "\n",
    "env = RestrictionWrapper(env, restrictor, return_object=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restricted_history = pd.DataFrame(columns=['episode', 'episode_step', 'agent', 'observation', 'reward', 'action'], index=(range(total_timesteps)))\n",
    "replay_buffer = ReplayBuffer(state_dim=flatdim(restrictor.observation_space), action_dim=flatdim(restrictor.action_space))\n",
    "\n",
    "# Do not render during training\n",
    "env.unwrapped.render_mode = None\n",
    "\n",
    "current_timestep = 0\n",
    "current_episode = 0\n",
    "t = tqdm(total=total_timesteps)\n",
    "\n",
    "while current_timestep < total_timesteps:\n",
    "    env.reset()\n",
    "    current_episode += 1\n",
    "    current_episode_timestep = 0\n",
    "    previous_restrictor_observation = None\n",
    "\n",
    "    for agent in env.agent_iter():\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "        if agent == 'restrictor_0':\n",
    "            if previous_restrictor_observation is not None:\n",
    "                restrictor.learn(previous_restrictor_observation, previous_restrictor_action, observation, reward, termination or truncation)\n",
    "\n",
    "            action = restrictor.act(observation)\n",
    "\n",
    "            previous_restrictor_observation = observation\n",
    "            previous_restrictor_action = action\n",
    "        else:\n",
    "            action = agents[agent].act(observation)\n",
    "\n",
    "        if termination or truncation:\n",
    "            action = None\n",
    "        else:\n",
    "\n",
    "        # print(f'{agent=}, {observation=}, {reward=}, {action=}')\n",
    "\n",
    "            restricted_history.loc[current_timestep] = pd.Series({'episode': current_episode, \n",
    "                                               'episode_step': current_episode_timestep, \n",
    "                                               'agent': agent,\n",
    "                                               'observation': observation, \n",
    "                                               'reward': reward, \n",
    "                                               'action': action}\n",
    "                                               )\n",
    "            \n",
    "            current_timestep += 1\n",
    "            current_episode_timestep += 1\n",
    "\n",
    "        env.step(action)\n",
    "\n",
    "        t.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrestricted_history = pd.DataFrame(columns=['episode', 'episode_step', 'agent', 'observation', 'reward', 'action'], index=(range(total_timesteps)))\n",
    "\n",
    "# Do not render during training\n",
    "env.unwrapped.render_mode = None\n",
    "\n",
    "current_timestep = 0\n",
    "current_episode = 0\n",
    "t = tqdm(total=total_timesteps)\n",
    "\n",
    "while current_timestep < total_timesteps:\n",
    "    env.reset()\n",
    "    current_episode += 1\n",
    "    current_episode_timestep = 0\n",
    "    previous_restrictor_observation = None\n",
    "\n",
    "    for agent in env.agent_iter():\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "        if agent == 'restrictor_0':\n",
    "            action = 14\n",
    "        else:\n",
    "            action = agents[agent].act(observation)\n",
    "\n",
    "        if termination or truncation:\n",
    "            action = None\n",
    "        else:\n",
    "            unrestricted_history.loc[current_timestep] = pd.Series({'episode': current_episode, \n",
    "                                               'episode_step': current_episode_timestep, \n",
    "                                               'agent': agent,\n",
    "                                               'observation': observation, \n",
    "                                               'reward': reward, \n",
    "                                               'action': action}\n",
    "                                               )\n",
    "            \n",
    "            current_timestep += 1\n",
    "            current_episode_timestep += 1\n",
    "\n",
    "        env.step(action)\n",
    "\n",
    "        t.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothen(data, kernel_size):\n",
    "    kernel = np.ones(kernel_size) / kernel_size\n",
    "\n",
    "    if data.ndim == 1:\n",
    "        return np.convolve(data, kernel, mode='same')\n",
    "    elif data.ndim == 2:\n",
    "        return np.array([np.convolve(col, kernel, mode='same') for col in data.T]).T\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restrictor_actions = restricted_history[restricted_history.agent == 'restrictor_0']['action'].astype(int)\n",
    "one_hot_restrictor_actions = np.eye(len(valid_edge_restrictions))[restrictor_actions.to_numpy().reshape(-1)]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "data = pd.DataFrame(smoothen(one_hot_restrictor_actions, kernel_size=5_000), index=restrictor_actions.index, columns=valid_edge_restrictions)\n",
    "lines = ax.plot(data.iloc[5000:-5000], color='gray', lw=1)\n",
    "# lines = ax.plot(restrictor_history.index, smoothen(one_hot_restrictor_actions, kernel_size=5_000), color='gray', lw=1)\n",
    "lines[11].set_color('red')\n",
    "ax.legend(labels=valid_edge_restrictions, loc='center left', bbox_to_anchor=(0.95, 0.5))\n",
    "ax.get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "# ax.get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "ax.set_ylabel('Frequency of restriction')\n",
    "ax.set_xlabel('Time step')\n",
    "\n",
    "fig.savefig('traffic-result-actions.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrestricted_social_welfare = unrestricted_history[unrestricted_history['agent'] == 'restrictor_0']['reward']\n",
    "unrestricted_data = pd.Series(smoothen(unrestricted_social_welfare.to_numpy().squeeze(), kernel_size=1000), index=unrestricted_social_welfare.index)\n",
    "\n",
    "restricted_social_welfare = restricted_history[restricted_history.agent == 'restrictor_0']['reward']\n",
    "restricted_data = pd.Series(smoothen(restricted_social_welfare.to_numpy().squeeze(), kernel_size=1000), index=restricted_social_welfare.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 3.5))\n",
    "ax.plot(restricted_data[1000:-1000] / 2, lw=1, color='green', label='Restricted social welfare')\n",
    "ax.plot(unrestricted_data[1000:-1000] / 2, lw=1, color='red', label='Unrestricted social welfare')\n",
    "\n",
    "ax.get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "ax.set_ylabel('Reward')\n",
    "ax.set_xlabel('Time step');\n",
    "ax.legend(loc='upper left')\n",
    "# ax.set_ylim(-1, -14)\n",
    "\n",
    "fig.savefig('traffic-result-reward.pdf', bbox_inches='tight')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record multiple runs for mean/std plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restrictor_action_runs = pd.DataFrame(index=restricted_history.index[restricted_history['agent'] == 'restrictor_0'])\n",
    "unrestricted_social_welfare_runs = pd.DataFrame(index=unrestricted_history.index[unrestricted_history['agent'] == 'restrictor_0'])\n",
    "restricted_social_welfare_runs = pd.DataFrame(index=restricted_history.index[restricted_history['agent'] == 'restrictor_0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recording"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restrictor_actions = restricted_history[restricted_history.agent == 'restrictor_0']['action'].astype(int)\n",
    "restrictor_action_runs = pd.concat([restrictor_action_runs, restrictor_actions], axis=1)\n",
    "\n",
    "restrictor_action_runs.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Social welfare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrestricted_social_welfare = unrestricted_history[unrestricted_history['agent'] == 'restrictor_0']['reward']\n",
    "restricted_social_welfare = restricted_history[unrestricted_history['agent'] == 'restrictor_0']['reward']\n",
    "unrestricted_social_welfare_runs = pd.concat([unrestricted_social_welfare_runs, unrestricted_social_welfare], axis=1)\n",
    "restricted_social_welfare_runs = pd.concat([restricted_social_welfare_runs, restricted_social_welfare], axis=1)\n",
    "\n",
    "unrestricted_social_welfare_runs.shape, restricted_social_welfare_runs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{restrictor_action_runs.shape=}, {unrestricted_social_welfare_runs.shape=}, {restricted_social_welfare_runs.shape=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 5_000\n",
    "one_hot_base = np.eye(len(valid_edge_restrictions))\n",
    "essential_action = 11\n",
    "\n",
    "one_hot_restrictor_action_runs = np.stack([one_hot_base[col] for col in restrictor_action_runs.to_numpy().T])\n",
    "\n",
    "one_hot_restrictor_actions_mean = one_hot_restrictor_action_runs.mean(axis=0)\n",
    "smooth_one_hot_restrictor_actions_mean = smoothen(one_hot_restrictor_actions_mean, kernel_size=kernel_size)\n",
    "data_mean = pd.DataFrame(smooth_one_hot_restrictor_actions_mean, index=restrictor_actions.index)[kernel_size:-kernel_size]\n",
    "\n",
    "one_hot_restrictor_actions_std = one_hot_restrictor_action_runs.std(axis=0)\n",
    "smooth_one_hot_restrictor_actions_std = smoothen(one_hot_restrictor_actions_std, kernel_size=kernel_size)\n",
    "data_std = pd.DataFrame(smooth_one_hot_restrictor_actions_std, index=restrictor_actions.index)[kernel_size:-kernel_size]\n",
    "\n",
    "# Downsampling by factor 50\n",
    "data_mean = data_mean[::50]\n",
    "data_std = data_std[::50]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=((5.8, 4.5)))\n",
    "\n",
    "lines = ax.plot(data_mean, color='gray', lw=1)\n",
    "lines[essential_action].set_color('red')\n",
    "\n",
    "for col in data_std:\n",
    "    ax.fill_between(data_std.index, data_mean[col] - data_std[col], data_mean[col] + data_std[col], facecolor = 'gray' if int(col) != essential_action else 'red', alpha=0.1 if int(col) != essential_action else 0.2)\n",
    "\n",
    "ax.legend(labels=valid_edge_restrictions, loc='center left', bbox_to_anchor=(0.95, 0.5))\n",
    "ax.get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "ax.set_ylabel('Frequency of restriction')\n",
    "ax.set_xlabel('Time step')\n",
    "\n",
    "fig.savefig('traffic-result-actions.pdf', bbox_inches='tight')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Social welfare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 1_000\n",
    "\n",
    "unrestricted_social_welfare_mean = unrestricted_social_welfare_runs.mean(axis=1)\n",
    "unrestricted_smooth_welfare_mean = pd.Series(smoothen(unrestricted_social_welfare_mean.to_numpy().squeeze(), kernel_size=kernel_size), index=unrestricted_social_welfare_mean.index)[kernel_size:-kernel_size].astype(float) / number_of_agents\n",
    "\n",
    "unrestricted_social_welfare_std = unrestricted_social_welfare_runs.std(axis=1)\n",
    "unrestricted_smooth_welfare_std = pd.Series(smoothen(unrestricted_social_welfare_std.to_numpy().squeeze(), kernel_size=kernel_size), index=unrestricted_social_welfare_std.index)[kernel_size:-kernel_size].astype(float) / number_of_agents\n",
    "\n",
    "restricted_social_welfare_mean = restricted_social_welfare_runs.mean(axis=1)\n",
    "restricted_smooth_welfare_mean = pd.Series(smoothen(restricted_social_welfare_mean.to_numpy().squeeze(), kernel_size=kernel_size), index=restricted_social_welfare_mean.index)[kernel_size:-kernel_size].astype(float) / number_of_agents\n",
    "\n",
    "restricted_social_welfare_std = restricted_social_welfare_runs.std(axis=1)\n",
    "restricted_smooth_welfare_std = pd.Series(smoothen(restricted_social_welfare_std.to_numpy().squeeze(), kernel_size=kernel_size), index=restricted_social_welfare_std.index)[kernel_size:-kernel_size].astype(float) / number_of_agents\n",
    "\n",
    "# Downsampling by factor 50\n",
    "unrestricted_smooth_welfare_mean = unrestricted_smooth_welfare_mean[::10]\n",
    "unrestricted_smooth_welfare_std = unrestricted_smooth_welfare_std[::10]\n",
    "restricted_smooth_welfare_mean = restricted_smooth_welfare_mean[::10]\n",
    "restricted_smooth_welfare_std = restricted_smooth_welfare_std[::10]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 3.5))\n",
    "\n",
    "ax.fill_between(unrestricted_smooth_welfare_std.index, unrestricted_smooth_welfare_mean - unrestricted_smooth_welfare_std, unrestricted_smooth_welfare_mean + unrestricted_smooth_welfare_std, facecolor='red', alpha=0.2)\n",
    "ax.fill_between(restricted_smooth_welfare_std.index, restricted_smooth_welfare_mean - restricted_smooth_welfare_std, restricted_smooth_welfare_mean + restricted_smooth_welfare_std, facecolor='green', alpha=0.2)\n",
    "\n",
    "ax.plot(unrestricted_smooth_welfare_mean, lw=1, color='red', label='Unrestricted social welfare')\n",
    "ax.plot(restricted_smooth_welfare_mean, lw=1, color='green', label='Restricted social welfare')\n",
    "\n",
    "ax.get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "ax.set_ylabel('Reward')\n",
    "ax.set_xlabel('Time step');\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "fig.savefig('traffic-result-reward.pdf', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
