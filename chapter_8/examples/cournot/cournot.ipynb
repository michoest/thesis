{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application: Learning optimal restrictions in a continuous-action game"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook corresponds to Section 5.1 of the paper \"Grams & Oesterle (forthcoming). _DRAMA at the PettingZoo: Dynamically Restricted Action Spaces for Multi-Agent Reinforcement Learning Frameworks_.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(f'{os.getcwd()}/../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.spaces import Box\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from src.wrapper import RestrictionWrapper\n",
    "from src.restrictors import IntervalUnionActionSpace\n",
    "\n",
    "from examples.utils import play\n",
    "from examples.cournot.env import NFGEnvironment\n",
    "from examples.cournot.agent import UnrestrictedCournotAgent, RestrictedCournotAgent\n",
    "from examples.cournot.restrictor import CournotRestrictor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the Cournot Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_price = 120\n",
    "cost = 12\n",
    "\n",
    "price_space = Box(0, maximum_price)\n",
    "observation_spaces = {'player_0': price_space, 'player_1': price_space}\n",
    "action_spaces = {'player_0': price_space, 'player_1': price_space}\n",
    "utilities = {\n",
    "    'player_0': (lambda actions: -actions['player_0'] ** 2 - actions['player_0'] * actions['player_1'] + (maximum_price - cost) * actions['player_0']), \n",
    "    'player_1': (lambda actions: -actions['player_1'] ** 2 - actions['player_0'] * actions['player_1'] + (maximum_price - cost) * actions['player_1'])}\n",
    "\n",
    "env = NFGEnvironment(observation_spaces, action_spaces, utilities, number_of_steps=100, render_mode='human')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Play without restrictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = {'player_0': UnrestrictedCournotAgent(maximum_price, cost).act, 'player_1': UnrestrictedCournotAgent(maximum_price, cost).act}\n",
    "trajectory = play(env, policies, max_iter=100, render_mode=None, record_trajectory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_reward = plt.subplots()\n",
    "ax_action = ax_reward.twinx()\n",
    "\n",
    "trajectory.groupby('agent')['reward'].plot(ax=ax_reward, style='-')\n",
    "\n",
    "trajectory.groupby('agent')['action'].plot(ax=ax_action, style='.', ylim=(0, maximum_price))\n",
    "\n",
    "# reward_grouped = trajectory.groupby('agent')['reward']\n",
    "# colors = {'player_0': 'blue', 'player_1': 'red'}\n",
    "# for key, group in reward_grouped:\n",
    "#     group.plot(ax=ax_reward, style='.-', lw=0.5, color=colors[key])\n",
    "\n",
    "# action_grouped = trajectory.groupby('agent')['action']\n",
    "# colors = {'player_0': 'green', 'player_1': 'black'}\n",
    "# for key, group in action_grouped:\n",
    "#     group.plot(ax=ax_action, style='.-', lw=0.5, ylim=(0, maximum_price), color=colors[key])\n",
    "\n",
    "ax_reward.xaxis.set_label_text('Iteration')\n",
    "ax_reward.yaxis.set_label_text('Reward')\n",
    "ax_action.yaxis.set_label_text('Action')\n",
    "\n",
    "ax_reward.legend(['Reward of Player 1', 'Reward of Player 2'], loc='upper right', bbox_to_anchor=(1, 0.85))\n",
    "ax_action.legend(['Action of Player 1', 'Action of Player 2'], loc='lower right', bbox_to_anchor=(1, 0.10));"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-learning restrictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run the environment with the `CournotRestrictor`, we see that it observes the agents and waits until their strategies converge. At this point the restrictor estimates the environment parameters (more concretely, the parameter `lambda := maximum_price - cost`) from the observed agent actions, and defines a suitable restriction. The agents then react to the restriction by changing their strategies. Eventually, the restriction gives a reward increase by approximately 12.5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_iterations = 100\n",
    "\n",
    "agents = [RestrictedCournotAgent(maximum_price, cost), RestrictedCournotAgent(maximum_price, cost)]\n",
    "restrictor = CournotRestrictor(Box(0, maximum_price, shape=(2, )), IntervalUnionActionSpace(Box(0, maximum_price)))\n",
    "wrapper = RestrictionWrapper(env, restrictor, restrictor_reward_fns={'restrictor_0': lambda env, rewards: rewards[env.agent_selection]}, return_object=True)\n",
    "\n",
    "# Use restrictor for all agents uniformly\n",
    "policies = {'player_0': agents[0].act, 'player_1': agents[1].act, 'restrictor_0': restrictor.act}\n",
    "\n",
    "# Run wrapped environment for 100 iterations\n",
    "trajectory = play(wrapper, policies, max_iter=number_of_iterations, render_mode=None, record_trajectory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_reward = plt.subplots(figsize=(7, 4))\n",
    "ax_action = ax_reward.twinx()\n",
    "\n",
    "trajectory.groupby('agent')['reward'].plot(style='.-', ax=ax_reward, lw=0.5)\n",
    "trajectory[trajectory['agent'] != 'restrictor_0'].groupby('agent')['action'].plot(style='.', ax=ax_action, ylim=(0, maximum_price))\n",
    "\n",
    "ax_reward.xaxis.set_label_text('Iteration')\n",
    "ax_reward.yaxis.set_label_text('Reward')\n",
    "ax_action.yaxis.set_label_text('Action')\n",
    "\n",
    "ax_reward.legend(['Reward of Player 1', 'Reward of Player 2', 'Social Welfare'], loc='upper right', bbox_to_anchor=(1, 0.9))\n",
    "ax_action.legend(['Action of Player 1', 'Action of Player 2'], loc='lower right', bbox_to_anchor=(1, 0))\n",
    "\n",
    "# Mark forbidden actions\n",
    "restrictor_actions = trajectory[trajectory['agent'] == 'restrictor_0']['action']\n",
    "begin_of_restriction = restrictor_actions.index[restrictor_actions.apply(lambda x: x.size < restrictor_actions.iloc[0].size)].min()\n",
    "[_, lower_bound], [upper_bound, _] = restrictor_actions.iloc[-1].intervals()\n",
    "\n",
    "ax_action.fill_between([begin_of_restriction, number_of_iterations], [lower_bound, lower_bound], [upper_bound, upper_bound], alpha=0.2, color='gray', lw=0)\n",
    "ax_action.text((number_of_iterations + begin_of_restriction) / 2, (lower_bound + upper_bound) / 2, '(Forbbiden actions)', ha='center', va='center', color='gray')\n",
    "ax_reward.get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "\n",
    "fig.savefig('cournot-result.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.restrictions import IntervalUnionRestriction\n",
    "\n",
    "r = IntervalUnionRestriction(Box(0, 120))\n",
    "r.contains(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
