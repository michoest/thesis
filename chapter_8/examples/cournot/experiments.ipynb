{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application: Learning optimal restrictions in a continuous-action game"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(f'{os.getcwd()}/../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.spaces import Box\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from drama.wrapper import RestrictionWrapper\n",
    "from drama.restrictors import IntervalUnionActionSpace\n",
    "\n",
    "from examples.utils import play\n",
    "from examples.cournot.env import NFGEnvironment\n",
    "from examples.cournot.agent import UnrestrictedCournotAgent, RestrictedCournotAgent\n",
    "from examples.cournot.restrictor import CournotRestrictor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_price = 120\n",
    "cost = 12\n",
    "\n",
    "price_space = Box(0, maximum_price)\n",
    "observation_spaces = {'agent_0': price_space, 'agent_1': price_space}\n",
    "action_spaces = {'agent_0': price_space, 'agent_1': price_space}\n",
    "utilities = {\n",
    "    'agent_0': (lambda actions: -actions['agent_0'] ** 2 - actions['agent_0'] * actions['agent_1'] + (maximum_price - cost) * actions['agent_0']), \n",
    "    'agent_1': (lambda actions: -actions['agent_1'] ** 2 - actions['agent_0'] * actions['agent_1'] + (maximum_price - cost) * actions['agent_1'])}\n",
    "\n",
    "env = NFGEnvironment(observation_spaces, action_spaces, utilities, number_of_steps=100, render_mode='human')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: Play without restrictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = {'agent_0': UnrestrictedCournotAgent(maximum_price, cost).act, 'agent_1': UnrestrictedCournotAgent(maximum_price, cost).act}\n",
    "trajectory = play(env, policies, max_iter=100, render_mode=None, record_trajectory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_reward = plt.subplots()\n",
    "ax_action = ax_reward.twinx()\n",
    "\n",
    "trajectory.groupby('agent')['reward'].plot(ax=ax_reward, style='-')\n",
    "\n",
    "trajectory.groupby('agent')['action'].plot(ax=ax_action, style='.', ylim=(0, maximum_price))\n",
    "\n",
    "ax_reward.xaxis.set_label_text('Iteration')\n",
    "ax_reward.yaxis.set_label_text('Reward')\n",
    "ax_action.yaxis.set_label_text('Action')\n",
    "\n",
    "ax_reward.legend(['Reward of agent 1', 'Reward of agent 2'], loc='upper right', bbox_to_anchor=(1, 0.85))\n",
    "ax_action.legend(['Action of agent 1', 'Action of agent 2'], loc='lower right', bbox_to_anchor=(1, 0.10));"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-learning restrictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run the environment with the `CournotRestrictor`, we see that it observes the agents and waits until their strategies converge. At this point the restrictor estimates the environment parameters (more concretely, the parameter `lambda := maximum_price - cost`) from the observed agent actions, and defines a suitable restriction. The agents then react to the restriction by changing their strategies. Eventually, the restriction gives a reward increase by approximately 12.5%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_iterations = 100\n",
    "\n",
    "agents = [RestrictedCournotAgent(maximum_price, cost), RestrictedCournotAgent(maximum_price, cost)]\n",
    "restrictor = CournotRestrictor(Box(0, maximum_price, shape=(2, )), IntervalUnionActionSpace(Box(0, maximum_price)))\n",
    "wrapper = RestrictionWrapper(env, restrictor, restrictor_reward_fns={'restrictor_0': lambda env, rewards: rewards[env.agent_selection]}, return_object=True)\n",
    "\n",
    "# Use restrictor for all agents uniformly\n",
    "policies = {'agent_0': agents[0].act, 'agent_1': agents[1].act, 'restrictor_0': restrictor.act}\n",
    "\n",
    "# Run wrapped environment for 100 iterations\n",
    "trajectory = play(wrapper, policies, max_iter=number_of_iterations, render_mode=None, record_trajectory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_reward = plt.subplots(figsize=(7, 4))\n",
    "ax_action = ax_reward.twinx()\n",
    "\n",
    "trajectory.groupby('agent')['reward'].plot(style='.-', ax=ax_reward, lw=0.5)\n",
    "trajectory[trajectory['agent'] != 'restrictor_0'].groupby('agent')['action'].plot(style='.', ax=ax_action, ylim=(0, maximum_price))\n",
    "\n",
    "ax_reward.xaxis.set_label_text('Iteration')\n",
    "ax_reward.yaxis.set_label_text('Reward')\n",
    "ax_action.yaxis.set_label_text('Action')\n",
    "\n",
    "ax_reward.legend(['Reward of agent 1', 'Reward of agent 2', 'Social Welfare'], loc='upper right', bbox_to_anchor=(1, 0.9))\n",
    "ax_action.legend(['Action of agent 1', 'Action of agent 2'], loc='lower right', bbox_to_anchor=(1, 0))\n",
    "\n",
    "# Mark forbidden actions\n",
    "restrictor_actions = trajectory[trajectory['agent'] == 'restrictor_0']['action']\n",
    "begin_of_restriction = restrictor_actions.index[restrictor_actions.apply(lambda x: x.size < restrictor_actions.iloc[0].size)].min()\n",
    "[_, lower_bound], [upper_bound, _] = restrictor_actions.iloc[-1].intervals()\n",
    "\n",
    "ax_action.fill_between([begin_of_restriction, number_of_iterations], [lower_bound, lower_bound], [upper_bound, upper_bound], alpha=0.2, color='gray', lw=0)\n",
    "ax_action.text((number_of_iterations + begin_of_restriction) / 2, (lower_bound + upper_bound) / 2, '(Forbbiden actions)', ha='center', va='center', color='gray')\n",
    "ax_reward.get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "\n",
    "fig.savefig('cournot-result.pdf', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
