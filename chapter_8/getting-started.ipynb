{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRAMA at the PettingZoo: Dynamically Restricted Action Spaces for Multi-Agent Reinforcement Learning Frameworks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates the basic functionality of restrictions, restrictors and restriction wrappers as described in _Oesterle et al. (2024): DRAMA at the PettingZoo: Dynamically Restricted Action Spaces for Multi-Agent Reinforcement Learning Frameworks_. More detailed examples can be found in the respective notebooks at `./examples/`, and the full documentation is available at https://drama-wrapper.readthedocs.io/."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from gymnasium.spaces import Discrete, Box, Space\n",
    "from pettingzoo import AECEnv\n",
    "from pettingzoo.classic import rps_v2\n",
    "from drama import DiscreteSetRestriction, IntervalUnionRestriction, DiscreteSetActionSpace, Restrictor, RestrictionWrapper, RestrictorActionSpace\n",
    "from examples.utils import play"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic usage of restrictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restrictions are subsets of `gym.Space`s. They are initialized with a base space and offer the same methods as a `gym.Space`, in particular `contains(x)` and `sample()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restriction = DiscreteSetRestriction(base_space=Discrete(10))\n",
    "print(restriction)\n",
    "restriction.remove(3)\n",
    "restriction.remove(5)\n",
    "print(restriction)\n",
    "restriction.add(2)\n",
    "restriction.add(3)\n",
    "print(restriction.contains(8))\n",
    "print(restriction.contains(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restriction = IntervalUnionRestriction(base_space=Box(0, 10))\n",
    "print(restriction)\n",
    "restriction.remove(3, 6)\n",
    "print(restriction)\n",
    "restriction.add(2, 4)\n",
    "print(restriction.contains(3))\n",
    "print(restriction.contains(5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Rock-Paper-Scissors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we build a restriction wrapper around the _Rock-Paper-Scissors_ environment (`rps_v2`) of `pettingzoo`. \n",
    "\n",
    "- The restrictor prevents each player from repeating an action, i.e., it observes the player's last move and excludes this action from the set of allowed actions.\n",
    "- The agents simply choose a random action from the allowed set.\n",
    "- The `RestrictionWrapper` wraps the environment (including its agents) and one or more `Restrictor`s. The agent-environment cycle (AEC) is extended by the wrapper such that a restriction is created before each agent's action by the respective restrictor. The agent then observes not only the original observation, but also the restriction, and can act according to this additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPSRestrictor(Restrictor):\n",
    "    player_mapping = {'player_0': 'player_1', 'player_1': 'player_0'}\n",
    "    \n",
    "    def preprocess_observation(self, env: AECEnv):\n",
    "        # Since the environment state is reset after each round, we need to get a player's \n",
    "        # previous action by looking at the _other_ player's observation\n",
    "        return env.unwrapped.observe(self.player_mapping[env.unwrapped.agent_selection]).item()\n",
    "    \n",
    "    def act(self, observation: Space) -> RestrictorActionSpace:\n",
    "        return DiscreteSetRestriction(base_space=self.action_space.base_space, allowed_actions=set(range(3)) - {observation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = rps_v2.env(num_actions=3, max_cycles=10, render_mode=None)\n",
    "restrictor = RPSRestrictor(Discrete(4), DiscreteSetActionSpace(base_space=Discrete(3)))\n",
    "wrapper = RestrictionWrapper(env, restrictor)\n",
    "\n",
    "def rps_random_policy(obs):\n",
    "    _, restriction = obs['observation'], obs['restriction']\n",
    "    return np.random.choice(restriction)\n",
    "\n",
    "policies = {'player_0': rps_random_policy, 'player_1': rps_random_policy, 'restrictor_0': restrictor.act}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We play the game for one episode (10 cycles) and observe that the AEC now consists of alternating restrictor and agent actions. The `play()` utility function records all observations, actions and rewards into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(wrapper, policies, record_trajectory=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6519815336a68311723c88fa17f06eeb3562085e823e45e5cc24f841b139f472"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
