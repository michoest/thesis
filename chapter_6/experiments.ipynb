{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## AAMAS 2022: Source Code for Submission #374\n",
    "\n",
    "### Instructions\n",
    "Using this notebook, you can reproduce the experiments shown in the submitted paper.\n",
    "Please read the paper before running the experiments as it defines the terminology\n",
    "required for the setup of the notebook.\n",
    "\n",
    "The notebook contains three cells:\n",
    "- _Setup_ - Contains all imports and function definitions\n",
    "- _Run_ - Runs the experiments and saves the results as Tensorboard log files\n",
    "- _Visualize_ - Reads existing Tensorboard log files and creates matplotlib charts\n",
    "\n",
    "To facilitate parallel execution on different machines, a single run only executes one\n",
    "configuration (this might take several hours to complete).\n",
    "\n",
    "Please execute the following steps to reproduce the experiments:\n",
    "1. Install all packages which are imported in the _Setup_ cell\n",
    "2. Choose an appropriate value for the log directory (marked with the comment `# CUSTOMIZE FOLDER`)\n",
    "3. Execute the _Setup_ cell\n",
    "4. Choose appropriate values for the configuration (marked with the comment `# CUSTOMIZE CONFIGURATION`); exemplary values are provided in the comments\n",
    "5. Execute the _Run_ cell\n",
    "6. Repeat steps 4 and 5 with different configurations \n",
    "7. Start Tensorboard and look for the five-letter experiment identifiers of all scenarios\n",
    "   (e.g., when the log folder is named *PPO_GMAS_4_GA_Environment_aabc7_00002_2_2021-09-28_13-46-08*,\n",
    "   the identifier is _aabc7_), or check the log directory for the identifiers\n",
    "8. Insert the identifiers in the variables marked with the comment `# CUSTOMIZE IDENTIFIERS`\n",
    "9. Execute the _Visualize_ cell\n",
    "\n",
    "The charts are both shown in the notebook and saved as PNG files in the subfolder _/charts/_.\n",
    "\n",
    "### Original experiments\n",
    "Unfortunately, it is not possible to include here the log files of the experiments shown in the paper \n",
    "(a single sample of one scenario in one configuration has ~50 MB, and there are 120 such files). \n",
    "Nevertheless, the log files can be accessed on a public repository after publication of the paper \n",
    "in order to provide full transparency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import glob\n",
    "from operator import itemgetter\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
    "from ray.rllib.models.tf.fcnet import FullyConnectedNetwork\n",
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "from ray.rllib.env import MultiAgentEnv\n",
    "\n",
    "from gym.spaces import Discrete, Box, Tuple, MultiDiscrete, Dict, MultiBinary\n",
    "\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "###########################################################\n",
    "# CUSTOMIZE FOLDER\n",
    "log_dir = '<log_dir>'\n",
    "###########################################################\n",
    "\n",
    "# Parametric-action agent model\n",
    "class ParametricAgentModel(TFModelV2):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name, *args, **kwargs):\n",
    "        super(ParametricAgentModel, self).__init__(obs_space, action_space, num_outputs, model_config, name, *args, **kwargs)\n",
    "\n",
    "        assert isinstance(action_space, Discrete), f'action_space is a {type(action_space)}, but should be Discrete!'\n",
    "\n",
    "        true_obs_shape = (3, )\n",
    "        action_embed_size = action_space.n\n",
    "\n",
    "        self.action_embed_model = FullyConnectedNetwork(Box(0, 1, shape=true_obs_shape), action_space, action_embed_size, model_config, name + '_action_embedding')\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        action_mask = input_dict['obs']['allowed_actions']\n",
    "        action_embedding, _ = self.action_embed_model({ 'obs': input_dict['obs']['obs'] })\n",
    "        intent_vector = tf.expand_dims(action_embedding, 1)\n",
    "        action_logits = tf.reduce_sum(intent_vector, axis=1)\n",
    "        inf_mask = tf.maximum(tf.math.log(action_mask), tf.float32.min)\n",
    "\n",
    "        return action_logits + inf_mask, state\n",
    "\n",
    "    def value_function(self):\n",
    "        return self.action_embed_model.value_function()\n",
    "\n",
    "# Passive governance policy (for UMAS scenario)\n",
    "class PassiveGovernancePolicy(Policy):\n",
    "    \"\"\"\n",
    "    Always allows all actions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space, action_space, config={}):\n",
    "        super().__init__(observation_space, action_space, config)\n",
    "\n",
    "        assert isinstance(action_space, MultiDiscrete), f'action_space is not MultiDiscrete ({type(action_space)})'\n",
    "        self.NUMBER_OF_ACTIONS = len(action_space.nvec)\n",
    "\n",
    "    def compute_actions(self, obs_batch, state_batches=None, prev_action_batch=None, prev_reward_batch=None,\n",
    "                        info_batch=None, episodes=None, **kwargs):\n",
    "\n",
    "        return [np.ones(self.NUMBER_OF_ACTIONS).astype(bool) for _ in obs_batch], state_batches, {}\n",
    "\n",
    "    def get_weights(self):\n",
    "        return {}\n",
    "\n",
    "    def set_weights(self, weights) -> None:\n",
    "        pass\n",
    "\n",
    "# Custom metrics logger\n",
    "class CustomMetricsLogger(DefaultCallbacks):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Unknown until on_episode_start\n",
    "        self.NUMBER_OF_AGENTS = self.NUMBER_OF_ACTIONS = self.NUMBER_OF_STEPS_PER_EPISODE = self.ALPHA = None\n",
    "\n",
    "    def on_episode_start(self, *, worker, base_env, policies, episode, env_index, **kwargs):\n",
    "        if self.NUMBER_OF_AGENTS is None:\n",
    "            env = base_env.get_unwrapped()[0]\n",
    "\n",
    "            self.NUMBER_OF_AGENTS = env.NUMBER_OF_AGENTS\n",
    "            self.NUMBER_OF_ACTIONS = env.NUMBER_OF_ACTIONS\n",
    "            self.NUMBER_OF_STEPS_PER_EPISODE = env.NUMBER_OF_STEPS_PER_EPISODE\n",
    "            self.ALPHA = env.ALPHA\n",
    "\n",
    "        episode.user_data['gov_info'] = []\n",
    "\n",
    "    def on_episode_step(self, *, worker, base_env, policies=None, episode, env_index, **kwargs):\n",
    "        env = base_env.get_unwrapped()[0]\n",
    "\n",
    "        if env.is_reward_step:\n",
    "            episode.user_data['gov_info'].append(np.array(episode.last_info_for('gov')))\n",
    "\n",
    "    def on_episode_end(self, *, worker, base_env, policies, episode, env_index, **kwargs):\n",
    "        gov_info = np.vstack(episode.user_data['gov_info'])\n",
    "        episode_state_reward, episode_restriction_reward, episode_degree_of_restriction = np.mean(gov_info, axis=0)\n",
    "\n",
    "        episode.custom_metrics['episode_degree_of_restriction/gov'] = episode_degree_of_restriction\n",
    "        episode.custom_metrics['episode_state_reward/gov'] = episode_state_reward\n",
    "        episode.custom_metrics['episode_restriction_reward/gov'] = episode_restriction_reward\n",
    "        episode.custom_metrics['episode_reward/gov'] = episode_state_reward + episode_restriction_reward\n",
    "\n",
    "    def on_postprocess_trajectory(self, *, worker, episode, agent_id, policy_id, policies, postprocessed_batch, original_batches, **kwargs):\n",
    "        if agent_id == 'gov':\n",
    "            rewards = postprocessed_batch['rewards']\n",
    "            infos = postprocessed_batch['infos']\n",
    "            assert len(rewards.shape) == 1\n",
    "\n",
    "            # Distribute rewards over all gov_actions in one environment step, not just the last one\n",
    "            for i in range(len(rewards)):\n",
    "                if rewards[i] != 0:\n",
    "                    rewards[max(0, i - self.NUMBER_OF_AGENTS + 1):(i+1)] = rewards[i] / self.NUMBER_OF_AGENTS\n",
    "\n",
    "# Dining diplomats environments\n",
    "class CC_Environment(MultiAgentEnv):\n",
    "    \"\"\"\n",
    "    Governance and agent reward is only 1 if all actions coincide.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict = {}):\n",
    "        assert 'NUMBER_OF_ACTIONS' in env_config\n",
    "        assert 'NUMBER_OF_AGENTS' in env_config\n",
    "        assert 'NUMBER_OF_STEPS_PER_EPISODE' in env_config\n",
    "        assert 'ALPHA' in env_config\n",
    "\n",
    "        self.NUMBER_OF_ACTIONS = env_config['NUMBER_OF_ACTIONS']\n",
    "        self.NUMBER_OF_STEPS_PER_EPISODE = env_config['NUMBER_OF_STEPS_PER_EPISODE']\n",
    "        self.ALPHA = env_config['ALPHA']\n",
    "        self.NUMBER_OF_AGENTS = env_config['NUMBER_OF_AGENTS']\n",
    "        self.agents = [str(i) for i in range(self.NUMBER_OF_AGENTS)]\n",
    "\n",
    "        self.agent_observations = { agent: np.array(np.mod([index - 1, index, index + 1], self.NUMBER_OF_AGENTS)) for index, agent in enumerate(self.agents) }\n",
    "\n",
    "        # Define obs and action spaces for agents\n",
    "        self.observation_space = Dict({ 'obs': Box(0, self.NUMBER_OF_ACTIONS - 1, shape=(3,)),\n",
    "                             'allowed_actions': MultiBinary(self.NUMBER_OF_ACTIONS) })\n",
    "        self.action_space = Discrete(self.NUMBER_OF_ACTIONS)\n",
    "\n",
    "        self.state = None\n",
    "        self.current_step = None\n",
    "        self.current_agent_index = None\n",
    "\n",
    "        self.allowed_actions = None\n",
    "\n",
    "        self.is_reward_step = None\n",
    "\n",
    "    def step(self, actions):\n",
    "        self.is_reward_step = False\n",
    "        if 'gov' in actions:\n",
    "            # Only the governance has acted\n",
    "            assert len(actions) == 1\n",
    "\n",
    "            # Governance action is a set of allowed actions; save it for later\n",
    "            self.allowed_actions[self.agents[self.current_agent_index]] = actions['gov'].astype(bool)\n",
    "            self.current_agent_index += 1\n",
    "        else:\n",
    "            # All agents have acted\n",
    "            assert len(actions) == len(self.agents)\n",
    "\n",
    "            # Execute transition\n",
    "            self.state = np.array([actions[agent] for agent in self.agents])\n",
    "\n",
    "            state_reward = 1 if np.all(self.state == self.state[0]) else 0\n",
    "            degree_of_restriction = np.mean([1 - (np.sum(allowed_actions) / self.NUMBER_OF_ACTIONS) for allowed_actions in self.allowed_actions.values()])\n",
    "            restriction_reward = -self.ALPHA * degree_of_restriction\n",
    "            gov_reward = state_reward + restriction_reward\n",
    "\n",
    "            self.current_step += 1\n",
    "            self.current_agent_index = 0\n",
    "            self.allowed_actions = { } # Could be removed for speed-up\n",
    "\n",
    "            # Governance needs to decide on allowed actions for first agent on the list (if episode is not over)\n",
    "            self.is_reward_step = True\n",
    "            return { 'gov': { 'state': self.state, 'obs': self.get_observation(self.agents[self.current_agent_index]) } }, \\\n",
    "                   { 'gov': gov_reward }, \\\n",
    "                   { '__all__': self.current_step >= self.NUMBER_OF_STEPS_PER_EPISODE }, \\\n",
    "                   { 'gov': (state_reward, restriction_reward, degree_of_restriction) }\n",
    "\n",
    "        # Governance needs to decide on allowed actions for next agent on the list\n",
    "        if self.current_agent_index < len(self.agents):\n",
    "            return { 'gov': { 'state': self.state, 'obs': self.get_observation(self.agents[self.current_agent_index]) } }, \\\n",
    "                   { 'gov': 0 }, \\\n",
    "                   { '__all__': False }, \\\n",
    "                   { 'gov': { } }\n",
    "\n",
    "        # Agents need to act\n",
    "        else:\n",
    "            agent_reward = 1 if np.all(self.state == self.state[0]) else 0\n",
    "\n",
    "            return { agent: { 'obs': self.get_observation(agent), 'allowed_actions': self.allowed_actions[agent] } for agent in self.agents }, \\\n",
    "                   { agent: agent_reward for agent in self.agents }, \\\n",
    "                   { '__all__': False }, \\\n",
    "                   { }\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.current_agent_index = 0\n",
    "\n",
    "        self.state = np.random.randint(0, self.NUMBER_OF_ACTIONS, (len(self.agents), ))\n",
    "\n",
    "        self.allowed_actions = { }\n",
    "\n",
    "        return { 'gov': { 'state': self.state, 'obs': self.get_observation(self.agents[self.current_agent_index]) } }\n",
    "\n",
    "    def get_observation(self, agent: str):\n",
    "        return self.state[self.agent_observations[agent]]\n",
    "\n",
    "class GMAS_Environment(MultiAgentEnv):\n",
    "    \"\"\"\n",
    "    Governance reward is only 1 if all actions coincide.\n",
    "    Agents get their rewards if their observations coincide.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict = {}):\n",
    "        assert 'NUMBER_OF_ACTIONS' in env_config\n",
    "        assert 'NUMBER_OF_AGENTS' in env_config\n",
    "        assert 'NUMBER_OF_STEPS_PER_EPISODE' in env_config\n",
    "        assert 'ALPHA' in env_config\n",
    "\n",
    "        self.NUMBER_OF_ACTIONS = env_config['NUMBER_OF_ACTIONS']\n",
    "        self.NUMBER_OF_STEPS_PER_EPISODE = env_config['NUMBER_OF_STEPS_PER_EPISODE']\n",
    "        self.ALPHA = env_config['ALPHA']\n",
    "        self.NUMBER_OF_AGENTS = env_config['NUMBER_OF_AGENTS']\n",
    "        self.agents = [str(i) for i in range(self.NUMBER_OF_AGENTS)]\n",
    "\n",
    "        self.agent_observations = { agent: np.array(np.mod([index - 1, index, index + 1], self.NUMBER_OF_AGENTS)) for index, agent in enumerate(self.agents) }\n",
    "\n",
    "        # Define obs and action spaces for agents\n",
    "        self.observation_space = Dict({ 'obs': Box(0, self.NUMBER_OF_ACTIONS - 1, shape=(3,)),\n",
    "                             'allowed_actions': MultiBinary(self.NUMBER_OF_ACTIONS) })\n",
    "        self.action_space = Discrete(self.NUMBER_OF_ACTIONS)\n",
    "\n",
    "        self.state = None\n",
    "        self.current_step = None\n",
    "        self.current_agent_index = None\n",
    "\n",
    "        self.allowed_actions = None\n",
    "\n",
    "        self.is_reward_step = None\n",
    "\n",
    "    def step(self, actions):\n",
    "        self.is_reward_step = False\n",
    "        if 'gov' in actions:\n",
    "            # Only the governance has acted\n",
    "            assert len(actions) == 1\n",
    "\n",
    "            # Governance action is a set of allowed actions; save it for later\n",
    "            self.allowed_actions[self.agents[self.current_agent_index]] = actions['gov'].astype(bool)\n",
    "            self.current_agent_index += 1\n",
    "        else:\n",
    "            # All agents have acted\n",
    "            assert len(actions) == len(self.agents)\n",
    "\n",
    "            # Execute transition\n",
    "            self.state = np.array([actions[agent] for agent in self.agents])\n",
    "\n",
    "            state_reward = 1 if np.all(self.state == self.state[0]) else 0\n",
    "            degree_of_restriction = np.mean([1 - (np.sum(allowed_actions) / self.NUMBER_OF_ACTIONS) for allowed_actions in self.allowed_actions.values()])\n",
    "            restriction_reward = -self.ALPHA * degree_of_restriction\n",
    "            gov_reward = state_reward + restriction_reward\n",
    "\n",
    "            self.current_step += 1\n",
    "            self.current_agent_index = 0\n",
    "            self.allowed_actions = { } # Could be removed for speed-up\n",
    "\n",
    "            # Governance needs to decide on allowed actions for first agent on the list (if episode is not over)\n",
    "            self.is_reward_step = True\n",
    "            return { 'gov': { 'state': self.state, 'obs': self.get_observation(self.agents[self.current_agent_index]) } }, \\\n",
    "                   { 'gov': gov_reward }, \\\n",
    "                   { '__all__': self.current_step >= self.NUMBER_OF_STEPS_PER_EPISODE }, \\\n",
    "                   { 'gov': (state_reward, restriction_reward, degree_of_restriction) }\n",
    "\n",
    "        # Governance needs to decide on allowed actions for next agent on the list\n",
    "        if self.current_agent_index < len(self.agents):\n",
    "            return { 'gov': { 'state': self.state, 'obs': self.get_observation(self.agents[self.current_agent_index]) } }, \\\n",
    "                   { 'gov': 0 }, \\\n",
    "                   { '__all__': False }, \\\n",
    "                   { 'gov': { } }\n",
    "\n",
    "        # Agents need to act\n",
    "        else:\n",
    "            agent_observations = { agent: self.state[self.agent_observations[agent]] for agent in self.agents }\n",
    "            agent_rewards = { agent: 1 if np.all(obs == obs[0]) else 0 for agent, obs in agent_observations.items() }\n",
    "\n",
    "            return { agent: { 'obs': self.get_observation(agent), 'allowed_actions': self.allowed_actions[agent] } for agent in self.agents }, \\\n",
    "                   { agent: agent_rewards[agent] for agent in self.agents }, \\\n",
    "                   { '__all__': False }, \\\n",
    "                   { }\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.current_agent_index = 0\n",
    "\n",
    "        self.state = np.random.randint(0, self.NUMBER_OF_ACTIONS, (len(self.agents), ))\n",
    "\n",
    "        self.allowed_actions = { }\n",
    "\n",
    "        return { 'gov': { 'state': self.state, 'obs': self.get_observation(self.agents[self.current_agent_index]) } }\n",
    "\n",
    "    def get_observation(self, agent: str):\n",
    "        return self.state[self.agent_observations[agent]]\n",
    "\n",
    "# Experiment without governance\n",
    "def run_experiment_without_governance(config):\n",
    "    NUMBER_OF_AGENTS = config['NUMBER_OF_AGENTS']\n",
    "    NUMBER_OF_ACTIONS = config['NUMBER_OF_ACTIONS']\n",
    "    NUMBER_OF_STEPS_PER_EPISODE = config['NUMBER_OF_STEPS_PER_EPISODE']\n",
    "    ALPHA = config['ALPHA']\n",
    "    ENV = config['ENV']\n",
    "    NUMBER_OF_TIMESTEPS = config['NUMBER_OF_TIMESTEPS']\n",
    "    NUMBER_OF_SAMPLES = config['NUMBER_OF_SAMPLES']\n",
    "    NAME = config['NAME']\n",
    "\n",
    "    gov_obs_space = Dict({ 'state': MultiDiscrete([NUMBER_OF_ACTIONS] * NUMBER_OF_AGENTS),\n",
    "                           'obs': Box(0, NUMBER_OF_ACTIONS - 1, shape=(3,))})\n",
    "    gov_action_space = MultiDiscrete([2] * NUMBER_OF_ACTIONS)\n",
    "\n",
    "    def policy_mapping_fn(agent_id, episode, **kwargs):\n",
    "        # TODO: Why is agent0 sometimes called?\n",
    "        if 'agent' in agent_id:\n",
    "            print(f'Invalid agent_id ({agent_id})!')\n",
    "\n",
    "        return agent_id if 'agent' not in agent_id else agent_id[5:]\n",
    "\n",
    "    run_config = {\n",
    "        'env': ENV,\n",
    "        'env_config': {\n",
    "          'NUMBER_OF_STEPS_PER_EPISODE': NUMBER_OF_STEPS_PER_EPISODE,\n",
    "          'NUMBER_OF_AGENTS': NUMBER_OF_AGENTS,\n",
    "          'NUMBER_OF_ACTIONS': NUMBER_OF_ACTIONS,\n",
    "          'ALPHA': ALPHA\n",
    "        },\n",
    "        'multiagent': {\n",
    "            'policies': {\n",
    "                **{str(i): (None, None, None, { }) for i in range(NUMBER_OF_AGENTS)},\n",
    "                'gov': (PassiveGovernancePolicy, gov_obs_space, gov_action_space, { })\n",
    "            },\n",
    "            'policy_mapping_fn': policy_mapping_fn,\n",
    "            'policies_to_train': [str(i) for i in range(NUMBER_OF_AGENTS)]\n",
    "        },\n",
    "        'callbacks': CustomMetricsLogger\n",
    "    }\n",
    "\n",
    "    return tune.run('PPO', verbose=1, config=run_config, stop={'timesteps_total': NUMBER_OF_TIMESTEPS},\n",
    "             num_samples=NUMBER_OF_SAMPLES, checkpoint_at_end=True,\n",
    "             name=NAME, local_dir=log_dir)\n",
    "\n",
    "# Experiment with governance\n",
    "def run_experiment_with_governance(config):\n",
    "    NUMBER_OF_AGENTS = config['NUMBER_OF_AGENTS']\n",
    "    NUMBER_OF_ACTIONS = config['NUMBER_OF_ACTIONS']\n",
    "    NUMBER_OF_STEPS_PER_EPISODE = config['NUMBER_OF_STEPS_PER_EPISODE']\n",
    "    ALPHA = config['ALPHA']\n",
    "    ENV = config['ENV']\n",
    "    NUMBER_OF_TIMESTEPS = config['NUMBER_OF_TIMESTEPS']\n",
    "    NUMBER_OF_SAMPLES = config['NUMBER_OF_SAMPLES']\n",
    "    NAME = config['NAME']\n",
    "\n",
    "    gov_obs_space = Dict({ 'state': MultiDiscrete([NUMBER_OF_ACTIONS] * NUMBER_OF_AGENTS),\n",
    "                           'obs': Box(0, NUMBER_OF_ACTIONS - 1, shape=(3,))})\n",
    "    gov_action_space = MultiDiscrete([2] * NUMBER_OF_ACTIONS)\n",
    "\n",
    "    def policy_mapping_fn(agent_id, episode, **kwargs):\n",
    "        # TODO: Why is agent0 sometimes called?\n",
    "        if 'agent' in agent_id:\n",
    "            print(f'Invalid agent_id ({agent_id})!')\n",
    "\n",
    "        return agent_id if 'agent' not in agent_id else agent_id[5:]\n",
    "\n",
    "    run_config = {\n",
    "        'env': ENV,\n",
    "        'env_config': {\n",
    "          'NUMBER_OF_STEPS_PER_EPISODE': NUMBER_OF_STEPS_PER_EPISODE,\n",
    "          'NUMBER_OF_AGENTS': NUMBER_OF_AGENTS,\n",
    "          'NUMBER_OF_ACTIONS': NUMBER_OF_ACTIONS,\n",
    "          'ALPHA': ALPHA\n",
    "        },\n",
    "        'multiagent': {\n",
    "            'policies': {\n",
    "                **{str(i): (None, None, None, { 'model': {'custom_model': ParametricAgentModel }, 'framework': 'tf' }) for i in range(NUMBER_OF_AGENTS)},\n",
    "                'gov': (None, gov_obs_space, gov_action_space, { })\n",
    "            },\n",
    "            'policy_mapping_fn': policy_mapping_fn,\n",
    "            'policies_to_train': [str(i) for i in range(NUMBER_OF_AGENTS)] + ['gov']\n",
    "        },\n",
    "        'callbacks': CustomMetricsLogger\n",
    "    }\n",
    "\n",
    "    return tune.run('PPO', verbose=1, config=run_config, stop={'timesteps_total': NUMBER_OF_TIMESTEPS},\n",
    "             num_samples=NUMBER_OF_SAMPLES, checkpoint_at_end=True,\n",
    "             name=NAME, local_dir=log_dir)\n",
    "\n",
    "# Chart creation\n",
    "def create_aamas_charts(ids):\n",
    "    outer_path = f'{log_dir}/aamas2022'\n",
    "\n",
    "    configurations = list(set(configuration for configuration, scenario in ids.keys()))\n",
    "    scenarios = list(set(scenario for configuration, scenario in ids.keys()))\n",
    "    kpis = ['governance_reward', 'degree_of_restriction']\n",
    "\n",
    "    metrics = {\n",
    "        'governance_reward': 'ray/tune/custom_metrics/episode_state_reward/gov_mean',\n",
    "        'degree_of_restriction': 'ray/tune/custom_metrics/episode_degree_of_restriction/gov_mean'\n",
    "    }\n",
    "\n",
    "    experiment_folders = { key: glob.glob(f'{outer_path}/*{id}*/') for key, id in ids.items() }\n",
    "    event_accumulators = { key: [EventAccumulator(f) for f in folders] for key, folders in experiment_folders.items() }\n",
    "\n",
    "    current, total = 1, sum(len(ea) for ea in event_accumulators.values())\n",
    "    for key, experiment in event_accumulators.items():\n",
    "        for ea in experiment:\n",
    "            print(f'\\rLoading EventAccumulator {current}/{total}...', end='')\n",
    "            ea.Reload()\n",
    "            current += 1\n",
    "\n",
    "    raw_data = { (configuration, scenario, kpi): [list(zip(*ea.Scalars(metrics[kpi]))) for ea in experiment] for kpi in kpis for (configuration, scenario), experiment in event_accumulators.items() }\n",
    "    processed_data = { key: { 'x': np.array(experiment[0][1]), 'y': [np.array(sample[2]) for sample in experiment] } for key, experiment in raw_data.items() }\n",
    "    final_data = { key: { 'x': experiment['x'], 'y': experiment['y'], 'mean': np.mean(experiment['y'], axis=0) } for key, experiment in processed_data.items() }\n",
    "\n",
    "    print(f'Finished!')\n",
    "\n",
    "    save_path = f'{log_dir}/aamas2022/charts'\n",
    "    plt.style.use({'figure.facecolor':'white'})\n",
    "\n",
    "    scenario_names = {\n",
    "        'umas': 'UMAS',\n",
    "        'cc': 'CC',\n",
    "        'gmas': 'GMAS'\n",
    "    }\n",
    "\n",
    "    colors = {\n",
    "        'umas': 'blue',\n",
    "        'cc': 'red',\n",
    "        'gmas': 'green'\n",
    "    }\n",
    "\n",
    "    for i, configuration in enumerate(configurations):\n",
    "        for j, kpi in enumerate(kpis):\n",
    "            for scenario in scenarios:\n",
    "                x, ys, mean = itemgetter('x', 'y', 'mean')(final_data[(configuration, scenario, kpi)])\n",
    "                color = colors[scenario]\n",
    "                for y in ys:\n",
    "                    plt.plot(x, y, color=color, alpha=0.4, linewidth=0.5)\n",
    "\n",
    "                plt.plot(x, mean, color=color, label=scenario_names[scenario])\n",
    "\n",
    "            plt.ticklabel_format(axis='x', useMathText=True)\n",
    "            plt.xlabel('$t$')\n",
    "            plt.legend()\n",
    "\n",
    "            plt.savefig(f'{save_path}/{configuration}_{kpi}.png', format='png', bbox_inches='tight')\n",
    "\n",
    "            plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Setup\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def run():\n",
    "    # Values for original experiments:\n",
    "    # Tiny: 'NUMBER_OF_AGENTS': 5, 'NUMBER_OF_ACTIONS': 3\n",
    "    # Small: 'NUMBER_OF_AGENTS': 10, 'NUMBER_OF_ACTIONS': 5\n",
    "    # Medium: 'NUMBER_OF_AGENTS': 15, 'NUMBER_OF_ACTIONS': 7\n",
    "    # Large: 'NUMBER_OF_AGENTS': 20, 'NUMBER_OF_ACTIONS': 10\n",
    "\n",
    "    # All other values stay the same (to see the general functionality and save time, \n",
    "    # you can reduce the number of steps and/or number of samples)\n",
    "\n",
    "    ###########################################################\n",
    "    # CUSTOMIZE CONFIGURATION\n",
    "    config = {\n",
    "        'NUMBER_OF_AGENTS': 10,\n",
    "        'NUMBER_OF_ACTIONS': 5,\n",
    "        'NUMBER_OF_STEPS_PER_EPISODE': 100,\n",
    "        'ALPHA': 0.0,\n",
    "        'NUMBER_OF_TIMESTEPS': 12_000,\n",
    "        'NUMBER_OF_SAMPLES': 3,\n",
    "        'NAME': 'aamas2022'\n",
    "    }\n",
    "    ###########################################################\n",
    "\n",
    "    # UMAS\n",
    "    config['ENV'] = GMAS_Environment\n",
    "    run_experiment_without_governance(config)\n",
    "\n",
    "    # CC\n",
    "    config['ENV'] = CC_Environment\n",
    "    run_experiment_with_governance(config)\n",
    "\n",
    "    # GMAS\n",
    "    config['ENV'] = GMAS_Environment\n",
    "    run_experiment_with_governance(config)\n",
    "\n",
    "run()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Run\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def visualize():\n",
    "    ###########################################################\n",
    "    # CUSTOMIZE IDENTIFIERS\n",
    "    ids = {\n",
    "        ('tiny', 'umas'): '<id>',\n",
    "        ('tiny', 'cc'): '<id>',\n",
    "        ('tiny', 'gmas'): '<id>',\n",
    "        ('small', 'umas'): '<id>',\n",
    "        ('small', 'cc'): '<id>',\n",
    "        ('small', 'gmas'): '<id>',\n",
    "        ('medium', 'umas'): '<id>',\n",
    "        ('medium', 'cc'): '<id>',\n",
    "        ('medium', 'gmas'): '<id>',\n",
    "        ('large', 'umas'): '<id>',\n",
    "        ('large', 'cc'): '<id>',\n",
    "        ('large', 'gmas'): '<id>'\n",
    "    }\n",
    "    ###########################################################\n",
    "\n",
    "    create_aamas_charts(ids)\n",
    "\n",
    "visualize()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Visualize\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}