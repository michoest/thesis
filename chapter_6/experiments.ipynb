{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Finding optimal restrictions via Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from operator import itemgetter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
    "from ray.rllib.models.tf.fcnet import FullyConnectedNetwork\n",
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
    "from ray.rllib.env import MultiAgentEnv\n",
    "\n",
    "from gym.spaces import Discrete, Box, Tuple, MultiDiscrete, Dict, MultiBinary\n",
    "\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParametricAgentModel(TFModelV2):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name, *args, **kwargs):\n",
    "        super(ParametricAgentModel, self).__init__(obs_space, action_space, num_outputs, model_config, name, *args, **kwargs)\n",
    "\n",
    "        assert isinstance(action_space, Discrete), f'action_space is a {type(action_space)}, but should be Discrete!'\n",
    "\n",
    "        true_obs_shape = (3, )\n",
    "        action_embed_size = action_space.n\n",
    "\n",
    "        self.action_embed_model = FullyConnectedNetwork(Box(0, 1, shape=true_obs_shape), action_space, action_embed_size, model_config, name + '_action_embedding')\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        action_mask = input_dict['obs']['allowed_actions']\n",
    "        action_embedding, _ = self.action_embed_model({ 'obs': input_dict['obs']['obs'] })\n",
    "        intent_vector = tf.expand_dims(action_embedding, 1)\n",
    "        action_logits = tf.reduce_sum(intent_vector, axis=1)\n",
    "        inf_mask = tf.maximum(tf.math.log(action_mask), tf.float32.min)\n",
    "\n",
    "        return action_logits + inf_mask, state\n",
    "\n",
    "    def value_function(self):\n",
    "        return self.action_embed_model.value_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Governance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PassiveGovernancePolicy(Policy):\n",
    "    \"\"\"\n",
    "    Always allows all actions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space, action_space, config={}):\n",
    "        super().__init__(observation_space, action_space, config)\n",
    "\n",
    "        assert isinstance(action_space, MultiDiscrete), f'action_space is not MultiDiscrete ({type(action_space)})'\n",
    "        self.NUMBER_OF_ACTIONS = len(action_space.nvec)\n",
    "\n",
    "    def compute_actions(self, obs_batch, state_batches=None, prev_action_batch=None, prev_reward_batch=None,\n",
    "                        info_batch=None, episodes=None, **kwargs):\n",
    "\n",
    "        return [np.ones(self.NUMBER_OF_ACTIONS).astype(bool) for _ in obs_batch], state_batches, {}\n",
    "\n",
    "    def get_weights(self):\n",
    "        return {}\n",
    "\n",
    "    def set_weights(self, weights) -> None:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMetricsLogger(DefaultCallbacks):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Unknown until on_episode_start\n",
    "        self.NUMBER_OF_AGENTS = self.NUMBER_OF_ACTIONS = self.NUMBER_OF_STEPS_PER_EPISODE = self.ALPHA = None\n",
    "\n",
    "    def on_episode_start(self, *, worker, base_env, policies, episode, env_index, **kwargs):\n",
    "        if self.NUMBER_OF_AGENTS is None:\n",
    "            env = base_env.get_unwrapped()[0]\n",
    "\n",
    "            self.NUMBER_OF_AGENTS = env.NUMBER_OF_AGENTS\n",
    "            self.NUMBER_OF_ACTIONS = env.NUMBER_OF_ACTIONS\n",
    "            self.NUMBER_OF_STEPS_PER_EPISODE = env.NUMBER_OF_STEPS_PER_EPISODE\n",
    "            self.ALPHA = env.ALPHA\n",
    "\n",
    "        episode.user_data['gov_info'] = []\n",
    "\n",
    "    def on_episode_step(self, *, worker, base_env, policies=None, episode, env_index, **kwargs):\n",
    "        env = base_env.get_unwrapped()[0]\n",
    "\n",
    "        if env.is_reward_step:\n",
    "            episode.user_data['gov_info'].append(np.array(episode.last_info_for('gov')))\n",
    "\n",
    "    def on_episode_end(self, *, worker, base_env, policies, episode, env_index, **kwargs):\n",
    "        gov_info = np.vstack(episode.user_data['gov_info'])\n",
    "        episode_state_reward, episode_restriction_reward, episode_degree_of_restriction = np.mean(gov_info, axis=0)\n",
    "\n",
    "        episode.custom_metrics['episode_degree_of_restriction/gov'] = episode_degree_of_restriction\n",
    "        episode.custom_metrics['episode_state_reward/gov'] = episode_state_reward\n",
    "        episode.custom_metrics['episode_restriction_reward/gov'] = episode_restriction_reward\n",
    "        episode.custom_metrics['episode_reward/gov'] = episode_state_reward + episode_restriction_reward\n",
    "\n",
    "    def on_postprocess_trajectory(self, *, worker, episode, agent_id, policy_id, policies, postprocessed_batch, original_batches, **kwargs):\n",
    "        if agent_id == 'gov':\n",
    "            rewards = postprocessed_batch['rewards']\n",
    "            infos = postprocessed_batch['infos']\n",
    "            assert len(rewards.shape) == 1\n",
    "\n",
    "            # Distribute rewards over all gov_actions in one environment step, not just the last one\n",
    "            for i in range(len(rewards)):\n",
    "                if rewards[i] != 0:\n",
    "                    rewards[max(0, i - self.NUMBER_OF_AGENTS + 1):(i+1)] = rewards[i] / self.NUMBER_OF_AGENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CC_Environment(MultiAgentEnv):\n",
    "    \"\"\"\n",
    "    Governance and agent reward is only 1 if all actions coincide.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict = {}):\n",
    "        assert 'NUMBER_OF_ACTIONS' in env_config\n",
    "        assert 'NUMBER_OF_AGENTS' in env_config\n",
    "        assert 'NUMBER_OF_STEPS_PER_EPISODE' in env_config\n",
    "        assert 'ALPHA' in env_config\n",
    "\n",
    "        self.NUMBER_OF_ACTIONS = env_config['NUMBER_OF_ACTIONS']\n",
    "        self.NUMBER_OF_STEPS_PER_EPISODE = env_config['NUMBER_OF_STEPS_PER_EPISODE']\n",
    "        self.ALPHA = env_config['ALPHA']\n",
    "        self.NUMBER_OF_AGENTS = env_config['NUMBER_OF_AGENTS']\n",
    "        self.agents = [str(i) for i in range(self.NUMBER_OF_AGENTS)]\n",
    "\n",
    "        self.agent_observations = { agent: np.array(np.mod([index - 1, index, index + 1], self.NUMBER_OF_AGENTS)) for index, agent in enumerate(self.agents) }\n",
    "\n",
    "        # Define obs and action spaces for agents\n",
    "        self.observation_space = Dict({ 'obs': Box(0, self.NUMBER_OF_ACTIONS - 1, shape=(3,)),\n",
    "                             'allowed_actions': MultiBinary(self.NUMBER_OF_ACTIONS) })\n",
    "        self.action_space = Discrete(self.NUMBER_OF_ACTIONS)\n",
    "\n",
    "        self.state = None\n",
    "        self.current_step = None\n",
    "        self.current_agent_index = None\n",
    "\n",
    "        self.allowed_actions = None\n",
    "\n",
    "        self.is_reward_step = None\n",
    "\n",
    "    def step(self, actions):\n",
    "        self.is_reward_step = False\n",
    "        if 'gov' in actions:\n",
    "            # Only the governance has acted\n",
    "            assert len(actions) == 1\n",
    "\n",
    "            # Governance action is a set of allowed actions; save it for later\n",
    "            self.allowed_actions[self.agents[self.current_agent_index]] = actions['gov'].astype(bool)\n",
    "            self.current_agent_index += 1\n",
    "        else:\n",
    "            # All agents have acted\n",
    "            assert len(actions) == len(self.agents)\n",
    "\n",
    "            # Execute transition\n",
    "            self.state = np.array([actions[agent] for agent in self.agents])\n",
    "\n",
    "            state_reward = 1 if np.all(self.state == self.state[0]) else 0\n",
    "            degree_of_restriction = np.mean([1 - (np.sum(allowed_actions) / self.NUMBER_OF_ACTIONS) for allowed_actions in self.allowed_actions.values()])\n",
    "            restriction_reward = -self.ALPHA * degree_of_restriction\n",
    "            gov_reward = state_reward + restriction_reward\n",
    "\n",
    "            self.current_step += 1\n",
    "            self.current_agent_index = 0\n",
    "            self.allowed_actions = { } # Could be removed for speed-up\n",
    "\n",
    "            # Governance needs to decide on allowed actions for first agent on the list (if episode is not over)\n",
    "            self.is_reward_step = True\n",
    "            return { 'gov': { 'state': self.state, 'obs': self.get_observation(self.agents[self.current_agent_index]) } }, \\\n",
    "                   { 'gov': gov_reward }, \\\n",
    "                   { '__all__': self.current_step >= self.NUMBER_OF_STEPS_PER_EPISODE }, \\\n",
    "                   { 'gov': (state_reward, restriction_reward, degree_of_restriction) }\n",
    "\n",
    "        # Governance needs to decide on allowed actions for next agent on the list\n",
    "        if self.current_agent_index < len(self.agents):\n",
    "            return { 'gov': { 'state': self.state, 'obs': self.get_observation(self.agents[self.current_agent_index]) } }, \\\n",
    "                   { 'gov': 0 }, \\\n",
    "                   { '__all__': False }, \\\n",
    "                   { 'gov': { } }\n",
    "\n",
    "        # Agents need to act\n",
    "        else:\n",
    "            agent_reward = 1 if np.all(self.state == self.state[0]) else 0\n",
    "\n",
    "            return { agent: { 'obs': self.get_observation(agent), 'allowed_actions': self.allowed_actions[agent] } for agent in self.agents }, \\\n",
    "                   { agent: agent_reward for agent in self.agents }, \\\n",
    "                   { '__all__': False }, \\\n",
    "                   { }\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.current_agent_index = 0\n",
    "\n",
    "        self.state = np.random.randint(0, self.NUMBER_OF_ACTIONS, (len(self.agents), ))\n",
    "\n",
    "        self.allowed_actions = { }\n",
    "\n",
    "        return { 'gov': { 'state': self.state, 'obs': self.get_observation(self.agents[self.current_agent_index]) } }\n",
    "\n",
    "    def get_observation(self, agent: str):\n",
    "        return self.state[self.agent_observations[agent]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMAS_Environment(MultiAgentEnv):\n",
    "    \"\"\"\n",
    "    Governance reward is only 1 if all actions coincide.\n",
    "    Agents get their rewards if their observations coincide.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict = {}):\n",
    "        assert 'NUMBER_OF_ACTIONS' in env_config\n",
    "        assert 'NUMBER_OF_AGENTS' in env_config\n",
    "        assert 'NUMBER_OF_STEPS_PER_EPISODE' in env_config\n",
    "        assert 'ALPHA' in env_config\n",
    "\n",
    "        self.NUMBER_OF_ACTIONS = env_config['NUMBER_OF_ACTIONS']\n",
    "        self.NUMBER_OF_STEPS_PER_EPISODE = env_config['NUMBER_OF_STEPS_PER_EPISODE']\n",
    "        self.ALPHA = env_config['ALPHA']\n",
    "        self.NUMBER_OF_AGENTS = env_config['NUMBER_OF_AGENTS']\n",
    "        self.agents = [str(i) for i in range(self.NUMBER_OF_AGENTS)]\n",
    "\n",
    "        self.agent_observations = { agent: np.array(np.mod([index - 1, index, index + 1], self.NUMBER_OF_AGENTS)) for index, agent in enumerate(self.agents) }\n",
    "\n",
    "        # Define obs and action spaces for agents\n",
    "        self.observation_space = Dict({ 'obs': Box(0, self.NUMBER_OF_ACTIONS - 1, shape=(3,)),\n",
    "                             'allowed_actions': MultiBinary(self.NUMBER_OF_ACTIONS) })\n",
    "        self.action_space = Discrete(self.NUMBER_OF_ACTIONS)\n",
    "\n",
    "        self.state = None\n",
    "        self.current_step = None\n",
    "        self.current_agent_index = None\n",
    "\n",
    "        self.allowed_actions = None\n",
    "\n",
    "        self.is_reward_step = None\n",
    "\n",
    "    def step(self, actions):\n",
    "        self.is_reward_step = False\n",
    "        if 'gov' in actions:\n",
    "            # Only the governance has acted\n",
    "            assert len(actions) == 1\n",
    "\n",
    "            # Governance action is a set of allowed actions; save it for later\n",
    "            self.allowed_actions[self.agents[self.current_agent_index]] = actions['gov'].astype(bool)\n",
    "            self.current_agent_index += 1\n",
    "        else:\n",
    "            # All agents have acted\n",
    "            assert len(actions) == len(self.agents)\n",
    "\n",
    "            # Execute transition\n",
    "            self.state = np.array([actions[agent] for agent in self.agents])\n",
    "\n",
    "            state_reward = 1 if np.all(self.state == self.state[0]) else 0\n",
    "            degree_of_restriction = np.mean([1 - (np.sum(allowed_actions) / self.NUMBER_OF_ACTIONS) for allowed_actions in self.allowed_actions.values()])\n",
    "            restriction_reward = -self.ALPHA * degree_of_restriction\n",
    "            gov_reward = state_reward + restriction_reward\n",
    "\n",
    "            self.current_step += 1\n",
    "            self.current_agent_index = 0\n",
    "            self.allowed_actions = { } # Could be removed for speed-up\n",
    "\n",
    "            # Governance needs to decide on allowed actions for first agent on the list (if episode is not over)\n",
    "            self.is_reward_step = True\n",
    "            return { 'gov': { 'state': self.state, 'obs': self.get_observation(self.agents[self.current_agent_index]) } }, \\\n",
    "                   { 'gov': gov_reward }, \\\n",
    "                   { '__all__': self.current_step >= self.NUMBER_OF_STEPS_PER_EPISODE }, \\\n",
    "                   { 'gov': (state_reward, restriction_reward, degree_of_restriction) }\n",
    "\n",
    "        # Governance needs to decide on allowed actions for next agent on the list\n",
    "        if self.current_agent_index < len(self.agents):\n",
    "            return { 'gov': { 'state': self.state, 'obs': self.get_observation(self.agents[self.current_agent_index]) } }, \\\n",
    "                   { 'gov': 0 }, \\\n",
    "                   { '__all__': False }, \\\n",
    "                   { 'gov': { } }\n",
    "\n",
    "        # Agents need to act\n",
    "        else:\n",
    "            agent_observations = { agent: self.state[self.agent_observations[agent]] for agent in self.agents }\n",
    "            agent_rewards = { agent: 1 if np.all(obs == obs[0]) else 0 for agent, obs in agent_observations.items() }\n",
    "\n",
    "            return { agent: { 'obs': self.get_observation(agent), 'allowed_actions': self.allowed_actions[agent] } for agent in self.agents }, \\\n",
    "                   { agent: agent_rewards[agent] for agent in self.agents }, \\\n",
    "                   { '__all__': False }, \\\n",
    "                   { }\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.current_agent_index = 0\n",
    "\n",
    "        self.state = np.random.randint(0, self.NUMBER_OF_ACTIONS, (len(self.agents), ))\n",
    "\n",
    "        self.allowed_actions = { }\n",
    "\n",
    "        return { 'gov': { 'state': self.state, 'obs': self.get_observation(self.agents[self.current_agent_index]) } }\n",
    "\n",
    "    def get_observation(self, agent: str):\n",
    "        return self.state[self.agent_observations[agent]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_without_governance(config):\n",
    "    NUMBER_OF_AGENTS = config['NUMBER_OF_AGENTS']\n",
    "    NUMBER_OF_ACTIONS = config['NUMBER_OF_ACTIONS']\n",
    "    NUMBER_OF_STEPS_PER_EPISODE = config['NUMBER_OF_STEPS_PER_EPISODE']\n",
    "    ALPHA = config['ALPHA']\n",
    "    ENV = config['ENV']\n",
    "    NUMBER_OF_TIMESTEPS = config['NUMBER_OF_TIMESTEPS']\n",
    "    NUMBER_OF_SAMPLES = config['NUMBER_OF_SAMPLES']\n",
    "    NAME = config['NAME']\n",
    "    LOG_DIR = config['LOG_DIR']\n",
    "\n",
    "    gov_obs_space = Dict({ 'state': MultiDiscrete([NUMBER_OF_ACTIONS] * NUMBER_OF_AGENTS),\n",
    "                           'obs': Box(0, NUMBER_OF_ACTIONS - 1, shape=(3,))})\n",
    "    gov_action_space = MultiDiscrete([2] * NUMBER_OF_ACTIONS)\n",
    "\n",
    "    def policy_mapping_fn(agent_id, episode, **kwargs):\n",
    "        # TODO: Why is agent0 sometimes called?\n",
    "        if 'agent' in agent_id:\n",
    "            print(f'Invalid agent_id ({agent_id})!')\n",
    "\n",
    "        return agent_id if 'agent' not in agent_id else agent_id[5:]\n",
    "\n",
    "    run_config = {\n",
    "        'env': ENV,\n",
    "        'env_config': {\n",
    "          'NUMBER_OF_STEPS_PER_EPISODE': NUMBER_OF_STEPS_PER_EPISODE,\n",
    "          'NUMBER_OF_AGENTS': NUMBER_OF_AGENTS,\n",
    "          'NUMBER_OF_ACTIONS': NUMBER_OF_ACTIONS,\n",
    "          'ALPHA': ALPHA\n",
    "        },\n",
    "        'multiagent': {\n",
    "            'policies': {\n",
    "                **{str(i): (None, None, None, { }) for i in range(NUMBER_OF_AGENTS)},\n",
    "                'gov': (PassiveGovernancePolicy, gov_obs_space, gov_action_space, { })\n",
    "            },\n",
    "            'policy_mapping_fn': policy_mapping_fn,\n",
    "            'policies_to_train': [str(i) for i in range(NUMBER_OF_AGENTS)]\n",
    "        },\n",
    "        'callbacks': CustomMetricsLogger\n",
    "    }\n",
    "\n",
    "    return tune.run('PPO', verbose=1, config=run_config, stop={'timesteps_total': NUMBER_OF_TIMESTEPS},\n",
    "             num_samples=NUMBER_OF_SAMPLES, checkpoint_at_end=True,\n",
    "             name=NAME, local_dir=LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_with_governance(config):\n",
    "    NUMBER_OF_AGENTS = config['NUMBER_OF_AGENTS']\n",
    "    NUMBER_OF_ACTIONS = config['NUMBER_OF_ACTIONS']\n",
    "    NUMBER_OF_STEPS_PER_EPISODE = config['NUMBER_OF_STEPS_PER_EPISODE']\n",
    "    ALPHA = config['ALPHA']\n",
    "    ENV = config['ENV']\n",
    "    NUMBER_OF_TIMESTEPS = config['NUMBER_OF_TIMESTEPS']\n",
    "    NUMBER_OF_SAMPLES = config['NUMBER_OF_SAMPLES']\n",
    "    NAME = config['NAME']\n",
    "    LOG_DIR = config['LOG_DIR']\n",
    "\n",
    "    gov_obs_space = Dict({ 'state': MultiDiscrete([NUMBER_OF_ACTIONS] * NUMBER_OF_AGENTS),\n",
    "                           'obs': Box(0, NUMBER_OF_ACTIONS - 1, shape=(3,))})\n",
    "    gov_action_space = MultiDiscrete([2] * NUMBER_OF_ACTIONS)\n",
    "\n",
    "    def policy_mapping_fn(agent_id, episode, **kwargs):\n",
    "        # TODO: Why is agent0 sometimes called?\n",
    "        if 'agent' in agent_id:\n",
    "            print(f'Invalid agent_id ({agent_id})!')\n",
    "\n",
    "        return agent_id if 'agent' not in agent_id else agent_id[5:]\n",
    "\n",
    "    run_config = {\n",
    "        'env': ENV,\n",
    "        'env_config': {\n",
    "          'NUMBER_OF_STEPS_PER_EPISODE': NUMBER_OF_STEPS_PER_EPISODE,\n",
    "          'NUMBER_OF_AGENTS': NUMBER_OF_AGENTS,\n",
    "          'NUMBER_OF_ACTIONS': NUMBER_OF_ACTIONS,\n",
    "          'ALPHA': ALPHA\n",
    "        },\n",
    "        'multiagent': {\n",
    "            'policies': {\n",
    "                **{str(i): (None, None, None, { 'model': {'custom_model': ParametricAgentModel }, 'framework': 'tf' }) for i in range(NUMBER_OF_AGENTS)},\n",
    "                'gov': (None, gov_obs_space, gov_action_space, { })\n",
    "            },\n",
    "            'policy_mapping_fn': policy_mapping_fn,\n",
    "            'policies_to_train': [str(i) for i in range(NUMBER_OF_AGENTS)] + ['gov']\n",
    "        },\n",
    "        'callbacks': CustomMetricsLogger\n",
    "    }\n",
    "\n",
    "    return tune.run('PPO', verbose=1, config=run_config, stop={'timesteps_total': NUMBER_OF_TIMESTEPS},\n",
    "             num_samples=NUMBER_OF_SAMPLES, checkpoint_at_end=True,\n",
    "             name=NAME, local_dir=LOG_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'Version' and 'ValueError'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# UMAS\u001b[39;00m\n\u001b[1;32m     13\u001b[0m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mENV\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m GMAS_Environment\n\u001b[0;32m---> 14\u001b[0m \u001b[43mrun_experiment_with_governance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 42\u001b[0m, in \u001b[0;36mrun_experiment_with_governance\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m agent_id \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124magent\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m agent_id \u001b[38;5;28;01melse\u001b[39;00m agent_id[\u001b[38;5;241m5\u001b[39m:]\n\u001b[1;32m     23\u001b[0m run_config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124menv\u001b[39m\u001b[38;5;124m'\u001b[39m: ENV,\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124menv_config\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m'\u001b[39m: CustomMetricsLogger\n\u001b[1;32m     40\u001b[0m }\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtune\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPPO\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtimesteps_total\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUMBER_OF_TIMESTEPS\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m         \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUMBER_OF_SAMPLES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_at_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m         \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLOG_DIR\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Professional/Promotion/InES/Research/PhD Thesis/thesis/venv/lib/python3.11/site-packages/ray/tune/tune.py:1002\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, storage_filesystem, search_alg, scheduler, checkpoint_config, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, resume, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, chdir_to_trial_dir, local_dir, _remote, _remote_string_queue, _entrypoint)\u001b[0m\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1001\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mis_finished() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m experiment_interrupted_event\u001b[38;5;241m.\u001b[39mis_set():\n\u001b[0;32m-> 1002\u001b[0m         \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1003\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m has_verbosity(Verbosity\u001b[38;5;241m.\u001b[39mV1_EXPERIMENT):\n\u001b[1;32m   1004\u001b[0m             _report_progress(runner, progress_reporter)\n",
      "File \u001b[0;32m~/Documents/Professional/Promotion/InES/Research/PhD Thesis/thesis/venv/lib/python3.11/site-packages/ray/tune/execution/tune_controller.py:722\u001b[0m, in \u001b[0;36mTuneController.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callbacks\u001b[38;5;241m.\u001b[39mon_step_begin(\n\u001b[1;32m    718\u001b[0m         iteration\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iteration, trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trials\n\u001b[1;32m    719\u001b[0m     )\n\u001b[1;32m    721\u001b[0m \u001b[38;5;66;03m# Ask searcher for more trials\u001b[39;00m\n\u001b[0;32m--> 722\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_update_trial_queue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;66;03m# Start actors for added trials\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_add_actors()\n",
      "File \u001b[0;32m~/Documents/Professional/Promotion/InES/Research/PhD Thesis/thesis/venv/lib/python3.11/site-packages/ray/tune/execution/tune_controller.py:822\u001b[0m, in \u001b[0;36mTuneController._maybe_update_trial_queue\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    817\u001b[0m dont_wait_for_trial \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    818\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pending_trials \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_running_trials \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_paused_trials\n\u001b[1;32m    819\u001b[0m )\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pending_trials) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_pending_trials:\n\u001b[0;32m--> 822\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_trial_queue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdont_wait_for_trial\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    823\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     dont_wait_for_trial \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Professional/Promotion/InES/Research/PhD Thesis/thesis/venv/lib/python3.11/site-packages/ray/tune/execution/tune_controller.py:625\u001b[0m, in \u001b[0;36mTuneController._update_trial_queue\u001b[0;34m(self, blocking, timeout)\u001b[0m\n\u001b[1;32m    622\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trial:\n\u001b[0;32m--> 625\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Professional/Promotion/InES/Research/PhD Thesis/thesis/venv/lib/python3.11/site-packages/ray/tune/execution/tune_controller.py:573\u001b[0m, in \u001b[0;36mTuneController.add_trial\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    570\u001b[0m     trial\u001b[38;5;241m.\u001b[39mresolve_config_placeholders(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_placeholder_resolvers)\n\u001b[1;32m    572\u001b[0m \u001b[38;5;66;03m# With trial.config resolved, create placement group factory if needed.\u001b[39;00m\n\u001b[0;32m--> 573\u001b[0m \u001b[43mtrial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_placement_group_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trials\u001b[38;5;241m.\u001b[39mappend(trial)\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trial\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m!=\u001b[39m Trial\u001b[38;5;241m.\u001b[39mTERMINATED:\n",
      "File \u001b[0;32m~/Documents/Professional/Promotion/InES/Research/PhD Thesis/thesis/venv/lib/python3.11/site-packages/ray/tune/experiment/trial.py:429\u001b[0m, in \u001b[0;36mTrial.create_placement_group_factory\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplacement_group_factory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    425\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_placement_group_factory \u001b[38;5;129;01mor\u001b[39;00m resource_dict_to_pg_factory()\n\u001b[1;32m    426\u001b[0m     )\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 429\u001b[0m default_resources \u001b[38;5;241m=\u001b[39m \u001b[43mtrainable_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_resource_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# If Trainable returns resources, do not allow manual override via\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# `resources_per_trial` by the user.\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m default_resources \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_placement_group_factory:\n",
      "File \u001b[0;32m~/Documents/Professional/Promotion/InES/Research/PhD Thesis/thesis/venv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:2383\u001b[0m, in \u001b[0;36mAlgorithm.default_resource_request\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m   2368\u001b[0m \u001b[38;5;129m@OverrideToImplementCustomLogic\u001b[39m\n\u001b[1;32m   2369\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   2370\u001b[0m \u001b[38;5;129m@override\u001b[39m(Trainable)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2380\u001b[0m \n\u001b[1;32m   2381\u001b[0m     \u001b[38;5;66;03m# Convenience config handles.\u001b[39;00m\n\u001b[1;32m   2382\u001b[0m     cf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_default_config()\u001b[38;5;241m.\u001b[39mupdate_from_dict(config)\n\u001b[0;32m-> 2383\u001b[0m     \u001b[43mcf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2384\u001b[0m     cf\u001b[38;5;241m.\u001b[39mfreeze()\n\u001b[1;32m   2386\u001b[0m     \u001b[38;5;66;03m# get evaluation config\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Professional/Promotion/InES/Research/PhD Thesis/thesis/venv/lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo.py:311\u001b[0m, in \u001b[0;36mPPOConfig.validate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;129m@override\u001b[39m(AlgorithmConfig)\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;66;03m# Call super's validation method.\u001b[39;00m\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;66;03m# Synchronous sampling, on-policy/PPO algos -> Check mismatches between\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;66;03m# `rollout_fragment_length` and `train_batch_size` to avoid user confusion.\u001b[39;00m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;66;03m# TODO (sven): Make rollout_fragment_length a property and create a private\u001b[39;00m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;66;03m#  attribute to store (possibly) user provided value (or \"auto\") in. Deprecate\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;66;03m#  `self.get_rollout_fragment_length()`.\u001b[39;00m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_train_batch_size_vs_rollout_fragment_length()\n",
      "File \u001b[0;32m~/Documents/Professional/Promotion/InES/Research/PhD Thesis/thesis/venv/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm_config.py:819\u001b[0m, in \u001b[0;36mAlgorithmConfig.validate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    810\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use `framework=tf` with the new API stack! Either switch to tf2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    811\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m via `config.framework(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf2\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)` OR disable the new API stack via \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`config.experimental(_enable_new_api_stack=False)`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    813\u001b[0m     )\n\u001b[1;32m    815\u001b[0m \u001b[38;5;66;03m# Check if torch framework supports torch.compile.\u001b[39;00m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    817\u001b[0m     _torch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    818\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework_str \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 819\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mversion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_torch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__version__\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mTORCH_COMPILE_REQUIRED_VERSION\u001b[49m\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtorch_compile_learner \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtorch_compile_worker)\n\u001b[1;32m    821\u001b[0m ):\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.compile is only supported from torch 2.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_if_correct_nn_framework_installed(_tf1, _tf, _torch)\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'Version' and 'ValueError'"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "        'NUMBER_OF_AGENTS': 5,\n",
    "        'NUMBER_OF_ACTIONS': 3,\n",
    "        'NUMBER_OF_STEPS_PER_EPISODE': 100,\n",
    "        'ALPHA': 0.0,\n",
    "        'NUMBER_OF_TIMESTEPS': 1_000,\n",
    "        'NUMBER_OF_SAMPLES': 3,\n",
    "        'NAME': 'chapter_6',\n",
    "        'LOG_DIR': '/Users/michael/Documents/Professional/Promotion/InES/Research/PhD Thesis/thesis/chapter_6/data'\n",
    "    }\n",
    "\n",
    "# UMAS\n",
    "config['ENV'] = GMAS_Environment\n",
    "run_experiment_with_governance(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        'NUMBER_OF_AGENTS': 10,\n",
    "        'NUMBER_OF_ACTIONS': 5,\n",
    "        'NUMBER_OF_STEPS_PER_EPISODE': 100,\n",
    "        'ALPHA': 0.0,\n",
    "        'NUMBER_OF_TIMESTEPS': 12_000,\n",
    "        'NUMBER_OF_SAMPLES': 3,\n",
    "        'NAME': 'aamas2022',\n",
    "        'LOG_DIR': './data/'\n",
    "    }\n",
    "\n",
    "# UMAS\n",
    "config['ENV'] = GMAS_Environment\n",
    "run_experiment_without_governance(config)\n",
    "\n",
    "# CC\n",
    "config['ENV'] = CC_Environment\n",
    "run_experiment_with_governance(config)\n",
    "\n",
    "# GMAS\n",
    "config['ENV'] = GMAS_Environment\n",
    "run_experiment_with_governance(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = {\n",
    "        ('tiny', 'umas'): '<id>',\n",
    "        ('tiny', 'cc'): '<id>',\n",
    "        ('tiny', 'gmas'): '<id>',\n",
    "        ('small', 'umas'): '<id>',\n",
    "        ('small', 'cc'): '<id>',\n",
    "        ('small', 'gmas'): '<id>',\n",
    "        ('medium', 'umas'): '<id>',\n",
    "        ('medium', 'cc'): '<id>',\n",
    "        ('medium', 'gmas'): '<id>',\n",
    "        ('large', 'umas'): '<id>',\n",
    "        ('large', 'cc'): '<id>',\n",
    "        ('large', 'gmas'): '<id>'\n",
    "    }\n",
    "\n",
    "def create_aamas_charts(ids):\n",
    "    outer_path = f'{log_dir}/aamas2022'\n",
    "\n",
    "    configurations = list(set(configuration for configuration, scenario in ids.keys()))\n",
    "    scenarios = list(set(scenario for configuration, scenario in ids.keys()))\n",
    "    kpis = ['governance_reward', 'degree_of_restriction']\n",
    "\n",
    "    metrics = {\n",
    "        'governance_reward': 'ray/tune/custom_metrics/episode_state_reward/gov_mean',\n",
    "        'degree_of_restriction': 'ray/tune/custom_metrics/episode_degree_of_restriction/gov_mean'\n",
    "    }\n",
    "\n",
    "    experiment_folders = { key: glob.glob(f'{outer_path}/*{id}*/') for key, id in ids.items() }\n",
    "    event_accumulators = { key: [EventAccumulator(f) for f in folders] for key, folders in experiment_folders.items() }\n",
    "\n",
    "    current, total = 1, sum(len(ea) for ea in event_accumulators.values())\n",
    "    for key, experiment in event_accumulators.items():\n",
    "        for ea in experiment:\n",
    "            print(f'\\rLoading EventAccumulator {current}/{total}...', end='')\n",
    "            ea.Reload()\n",
    "            current += 1\n",
    "\n",
    "    raw_data = { (configuration, scenario, kpi): [list(zip(*ea.Scalars(metrics[kpi]))) for ea in experiment] for kpi in kpis for (configuration, scenario), experiment in event_accumulators.items() }\n",
    "    processed_data = { key: { 'x': np.array(experiment[0][1]), 'y': [np.array(sample[2]) for sample in experiment] } for key, experiment in raw_data.items() }\n",
    "    final_data = { key: { 'x': experiment['x'], 'y': experiment['y'], 'mean': np.mean(experiment['y'], axis=0) } for key, experiment in processed_data.items() }\n",
    "\n",
    "    print(f'Finished!')\n",
    "\n",
    "    save_path = f'{log_dir}/aamas2022/charts'\n",
    "    plt.style.use({'figure.facecolor':'white'})\n",
    "\n",
    "    scenario_names = {\n",
    "        'umas': 'UMAS',\n",
    "        'cc': 'CC',\n",
    "        'gmas': 'GMAS'\n",
    "    }\n",
    "\n",
    "    colors = {\n",
    "        'umas': 'blue',\n",
    "        'cc': 'red',\n",
    "        'gmas': 'green'\n",
    "    }\n",
    "\n",
    "    for i, configuration in enumerate(configurations):\n",
    "        for j, kpi in enumerate(kpis):\n",
    "            for scenario in scenarios:\n",
    "                x, ys, mean = itemgetter('x', 'y', 'mean')(final_data[(configuration, scenario, kpi)])\n",
    "                color = colors[scenario]\n",
    "                for y in ys:\n",
    "                    plt.plot(x, y, color=color, alpha=0.4, linewidth=0.5)\n",
    "\n",
    "                plt.plot(x, mean, color=color, label=scenario_names[scenario])\n",
    "\n",
    "            plt.ticklabel_format(axis='x', useMathText=True)\n",
    "            plt.xlabel('$t$')\n",
    "            plt.legend()\n",
    "\n",
    "            plt.savefig(f'{save_path}/{configuration}_{kpi}.png', format='png', bbox_inches='tight')\n",
    "\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deprecated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Instructions\n",
    "Using this notebook, you can reproduce the experiments shown in the submitted paper.\n",
    "Please read the paper before running the experiments as it defines the terminology\n",
    "required for the setup of the notebook.\n",
    "\n",
    "The notebook contains three cells:\n",
    "- _Setup_ - Contains all imports and function definitions\n",
    "- _Run_ - Runs the experiments and saves the results as Tensorboard log files\n",
    "- _Visualize_ - Reads existing Tensorboard log files and creates matplotlib charts\n",
    "\n",
    "To facilitate parallel execution on different machines, a single run only executes one\n",
    "configuration (this might take several hours to complete).\n",
    "\n",
    "Please execute the following steps to reproduce the experiments:\n",
    "1. Install all packages which are imported in the _Setup_ cell\n",
    "2. Choose an appropriate value for the log directory (marked with the comment `# CUSTOMIZE FOLDER`)\n",
    "3. Execute the _Setup_ cell\n",
    "4. Choose appropriate values for the configuration (marked with the comment `# CUSTOMIZE CONFIGURATION`); exemplary values are provided in the comments\n",
    "5. Execute the _Run_ cell\n",
    "6. Repeat steps 4 and 5 with different configurations \n",
    "7. Start Tensorboard and look for the five-letter experiment identifiers of all scenarios\n",
    "   (e.g., when the log folder is named *PPO_GMAS_4_GA_Environment_aabc7_00002_2_2021-09-28_13-46-08*,\n",
    "   the identifier is _aabc7_), or check the log directory for the identifiers\n",
    "8. Insert the identifiers in the variables marked with the comment `# CUSTOMIZE IDENTIFIERS`\n",
    "9. Execute the _Visualize_ cell\n",
    "\n",
    "The charts are both shown in the notebook and saved as PNG files in the subfolder _/charts/_.\n",
    "\n",
    "### Original experiments\n",
    "Unfortunately, it is not possible to include here the log files of the experiments shown in the paper \n",
    "(a single sample of one scenario in one configuration has ~50 MB, and there are 120 such files). \n",
    "Nevertheless, the log files can be accessed on a public repository after publication of the paper \n",
    "in order to provide full transparency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Setup\n"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from operator import itemgetter\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
    "from ray.rllib.models.tf.fcnet import FullyConnectedNetwork\n",
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "from ray.rllib.env import MultiAgentEnv\n",
    "\n",
    "from gym.spaces import Discrete, Box, Tuple, MultiDiscrete, Dict, MultiBinary\n",
    "\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "###########################################################\n",
    "# CUSTOMIZE FOLDER\n",
    "log_dir = '<log_dir>'\n",
    "###########################################################\n",
    "\n",
    "# Parametric-action agent model\n",
    "class ParametricAgentModel(TFModelV2):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name, *args, **kwargs):\n",
    "        super(ParametricAgentModel, self).__init__(obs_space, action_space, num_outputs, model_config, name, *args, **kwargs)\n",
    "\n",
    "        assert isinstance(action_space, Discrete), f'action_space is a {type(action_space)}, but should be Discrete!'\n",
    "\n",
    "        true_obs_shape = (3, )\n",
    "        action_embed_size = action_space.n\n",
    "\n",
    "        self.action_embed_model = FullyConnectedNetwork(Box(0, 1, shape=true_obs_shape), action_space, action_embed_size, model_config, name + '_action_embedding')\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        action_mask = input_dict['obs']['allowed_actions']\n",
    "        action_embedding, _ = self.action_embed_model({ 'obs': input_dict['obs']['obs'] })\n",
    "        intent_vector = tf.expand_dims(action_embedding, 1)\n",
    "        action_logits = tf.reduce_sum(intent_vector, axis=1)\n",
    "        inf_mask = tf.maximum(tf.math.log(action_mask), tf.float32.min)\n",
    "\n",
    "        return action_logits + inf_mask, state\n",
    "\n",
    "    def value_function(self):\n",
    "        return self.action_embed_model.value_function()\n",
    "\n",
    "# Passive governance policy (for UMAS scenario)\n",
    "class PassiveGovernancePolicy(Policy):\n",
    "    \"\"\"\n",
    "    Always allows all actions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space, action_space, config={}):\n",
    "        super().__init__(observation_space, action_space, config)\n",
    "\n",
    "        assert isinstance(action_space, MultiDiscrete), f'action_space is not MultiDiscrete ({type(action_space)})'\n",
    "        self.NUMBER_OF_ACTIONS = len(action_space.nvec)\n",
    "\n",
    "    def compute_actions(self, obs_batch, state_batches=None, prev_action_batch=None, prev_reward_batch=None,\n",
    "                        info_batch=None, episodes=None, **kwargs):\n",
    "\n",
    "        return [np.ones(self.NUMBER_OF_ACTIONS).astype(bool) for _ in obs_batch], state_batches, {}\n",
    "\n",
    "    def get_weights(self):\n",
    "        return {}\n",
    "\n",
    "    def set_weights(self, weights) -> None:\n",
    "        pass\n",
    "\n",
    "# Custom metrics logger\n",
    "class CustomMetricsLogger(DefaultCallbacks):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Unknown until on_episode_start\n",
    "        self.NUMBER_OF_AGENTS = self.NUMBER_OF_ACTIONS = self.NUMBER_OF_STEPS_PER_EPISODE = self.ALPHA = None\n",
    "\n",
    "    def on_episode_start(self, *, worker, base_env, policies, episode, env_index, **kwargs):\n",
    "        if self.NUMBER_OF_AGENTS is None:\n",
    "            env = base_env.get_unwrapped()[0]\n",
    "\n",
    "            self.NUMBER_OF_AGENTS = env.NUMBER_OF_AGENTS\n",
    "            self.NUMBER_OF_ACTIONS = env.NUMBER_OF_ACTIONS\n",
    "            self.NUMBER_OF_STEPS_PER_EPISODE = env.NUMBER_OF_STEPS_PER_EPISODE\n",
    "            self.ALPHA = env.ALPHA\n",
    "\n",
    "        episode.user_data['gov_info'] = []\n",
    "\n",
    "    def on_episode_step(self, *, worker, base_env, policies=None, episode, env_index, **kwargs):\n",
    "        env = base_env.get_unwrapped()[0]\n",
    "\n",
    "        if env.is_reward_step:\n",
    "            episode.user_data['gov_info'].append(np.array(episode.last_info_for('gov')))\n",
    "\n",
    "    def on_episode_end(self, *, worker, base_env, policies, episode, env_index, **kwargs):\n",
    "        gov_info = np.vstack(episode.user_data['gov_info'])\n",
    "        episode_state_reward, episode_restriction_reward, episode_degree_of_restriction = np.mean(gov_info, axis=0)\n",
    "\n",
    "        episode.custom_metrics['episode_degree_of_restriction/gov'] = episode_degree_of_restriction\n",
    "        episode.custom_metrics['episode_state_reward/gov'] = episode_state_reward\n",
    "        episode.custom_metrics['episode_restriction_reward/gov'] = episode_restriction_reward\n",
    "        episode.custom_metrics['episode_reward/gov'] = episode_state_reward + episode_restriction_reward\n",
    "\n",
    "    def on_postprocess_trajectory(self, *, worker, episode, agent_id, policy_id, policies, postprocessed_batch, original_batches, **kwargs):\n",
    "        if agent_id == 'gov':\n",
    "            rewards = postprocessed_batch['rewards']\n",
    "            infos = postprocessed_batch['infos']\n",
    "            assert len(rewards.shape) == 1\n",
    "\n",
    "            # Distribute rewards over all gov_actions in one environment step, not just the last one\n",
    "            for i in range(len(rewards)):\n",
    "                if rewards[i] != 0:\n",
    "                    rewards[max(0, i - self.NUMBER_OF_AGENTS + 1):(i+1)] = rewards[i] / self.NUMBER_OF_AGENTS\n",
    "\n",
    "# Dining diplomats environments\n",
    "class CC_Environment(MultiAgentEnv):\n",
    "    \"\"\"\n",
    "    Governance and agent reward is only 1 if all actions coincide.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict = {}):\n",
    "        assert 'NUMBER_OF_ACTIONS' in env_config\n",
    "        assert 'NUMBER_OF_AGENTS' in env_config\n",
    "        assert 'NUMBER_OF_STEPS_PER_EPISODE' in env_config\n",
    "        assert 'ALPHA' in env_config\n",
    "\n",
    "        self.NUMBER_OF_ACTIONS = env_config['NUMBER_OF_ACTIONS']\n",
    "        self.NUMBER_OF_STEPS_PER_EPISODE = env_config['NUMBER_OF_STEPS_PER_EPISODE']\n",
    "        self.ALPHA = env_config['ALPHA']\n",
    "        self.NUMBER_OF_AGENTS = env_config['NUMBER_OF_AGENTS']\n",
    "        self.agents = [str(i) for i in range(self.NUMBER_OF_AGENTS)]\n",
    "\n",
    "        self.agent_observations = { agent: np.array(np.mod([index - 1, index, index + 1], self.NUMBER_OF_AGENTS)) for index, agent in enumerate(self.agents) }\n",
    "\n",
    "        # Define obs and action spaces for agents\n",
    "        self.observation_space = Dict({ 'obs': Box(0, self.NUMBER_OF_ACTIONS - 1, shape=(3,)),\n",
    "                             'allowed_actions': MultiBinary(self.NUMBER_OF_ACTIONS) })\n",
    "        self.action_space = Discrete(self.NUMBER_OF_ACTIONS)\n",
    "\n",
    "        self.state = None\n",
    "        self.current_step = None\n",
    "        self.current_agent_index = None\n",
    "\n",
    "        self.allowed_actions = None\n",
    "\n",
    "        self.is_reward_step = None\n",
    "\n",
    "    def step(self, actions):\n",
    "        self.is_reward_step = False\n",
    "        if 'gov' in actions:\n",
    "            # Only the governance has acted\n",
    "            assert len(actions) == 1\n",
    "\n",
    "            # Governance action is a set of allowed actions; save it for later\n",
    "            self.allowed_actions[self.agents[self.current_agent_index]] = actions['gov'].astype(bool)\n",
    "            self.current_agent_index += 1\n",
    "        else:\n",
    "            # All agents have acted\n",
    "            assert len(actions) == len(self.agents)\n",
    "\n",
    "            # Execute transition\n",
    "            self.state = np.array([actions[agent] for agent in self.agents])\n",
    "\n",
    "            state_reward = 1 if np.all(self.state == self.state[0]) else 0\n",
    "            degree_of_restriction = np.mean([1 - (np.sum(allowed_actions) / self.NUMBER_OF_ACTIONS) for allowed_actions in self.allowed_actions.values()])\n",
    "            restriction_reward = -self.ALPHA * degree_of_restriction\n",
    "            gov_reward = state_reward + restriction_reward\n",
    "\n",
    "            self.current_step += 1\n",
    "            self.current_agent_index = 0\n",
    "            self.allowed_actions = { } # Could be removed for speed-up\n",
    "\n",
    "            # Governance needs to decide on allowed actions for first agent on the list (if episode is not over)\n",
    "            self.is_reward_step = True\n",
    "            return { 'gov': { 'state': self.state, 'obs': self.get_observation(self.agents[self.current_agent_index]) } }, \\\n",
    "                   { 'gov': gov_reward }, \\\n",
    "                   { '__all__': self.current_step >= self.NUMBER_OF_STEPS_PER_EPISODE }, \\\n",
    "                   { 'gov': (state_reward, restriction_reward, degree_of_restriction) }\n",
    "\n",
    "        # Governance needs to decide on allowed actions for next agent on the list\n",
    "        if self.current_agent_index < len(self.agents):\n",
    "            return { 'gov': { 'state': self.state, 'obs': self.get_observation(self.agents[self.current_agent_index]) } }, \\\n",
    "                   { 'gov': 0 }, \\\n",
    "                   { '__all__': False }, \\\n",
    "                   { 'gov': { } }\n",
    "\n",
    "        # Agents need to act\n",
    "        else:\n",
    "            agent_reward = 1 if np.all(self.state == self.state[0]) else 0\n",
    "\n",
    "            return { agent: { 'obs': self.get_observation(agent), 'allowed_actions': self.allowed_actions[agent] } for agent in self.agents }, \\\n",
    "                   { agent: agent_reward for agent in self.agents }, \\\n",
    "                   { '__all__': False }, \\\n",
    "                   { }\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.current_agent_index = 0\n",
    "\n",
    "        self.state = np.random.randint(0, self.NUMBER_OF_ACTIONS, (len(self.agents), ))\n",
    "\n",
    "        self.allowed_actions = { }\n",
    "\n",
    "        return { 'gov': { 'state': self.state, 'obs': self.get_observation(self.agents[self.current_agent_index]) } }\n",
    "\n",
    "    def get_observation(self, agent: str):\n",
    "        return self.state[self.agent_observations[agent]]\n",
    "\n",
    "class GMAS_Environment(MultiAgentEnv):\n",
    "    \"\"\"\n",
    "    Governance reward is only 1 if all actions coincide.\n",
    "    Agents get their rewards if their observations coincide.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict = {}):\n",
    "        assert 'NUMBER_OF_ACTIONS' in env_config\n",
    "        assert 'NUMBER_OF_AGENTS' in env_config\n",
    "        assert 'NUMBER_OF_STEPS_PER_EPISODE' in env_config\n",
    "        assert 'ALPHA' in env_config\n",
    "\n",
    "        self.NUMBER_OF_ACTIONS = env_config['NUMBER_OF_ACTIONS']\n",
    "        self.NUMBER_OF_STEPS_PER_EPISODE = env_config['NUMBER_OF_STEPS_PER_EPISODE']\n",
    "        self.ALPHA = env_config['ALPHA']\n",
    "        self.NUMBER_OF_AGENTS = env_config['NUMBER_OF_AGENTS']\n",
    "        self.agents = [str(i) for i in range(self.NUMBER_OF_AGENTS)]\n",
    "\n",
    "        self.agent_observations = { agent: np.array(np.mod([index - 1, index, index + 1], self.NUMBER_OF_AGENTS)) for index, agent in enumerate(self.agents) }\n",
    "\n",
    "        # Define obs and action spaces for agents\n",
    "        self.observation_space = Dict({ 'obs': Box(0, self.NUMBER_OF_ACTIONS - 1, shape=(3,)),\n",
    "                             'allowed_actions': MultiBinary(self.NUMBER_OF_ACTIONS) })\n",
    "        self.action_space = Discrete(self.NUMBER_OF_ACTIONS)\n",
    "\n",
    "        self.state = None\n",
    "        self.current_step = None\n",
    "        self.current_agent_index = None\n",
    "\n",
    "        self.allowed_actions = None\n",
    "\n",
    "        self.is_reward_step = None\n",
    "\n",
    "    def step(self, actions):\n",
    "        self.is_reward_step = False\n",
    "        if 'gov' in actions:\n",
    "            # Only the governance has acted\n",
    "            assert len(actions) == 1\n",
    "\n",
    "            # Governance action is a set of allowed actions; save it for later\n",
    "            self.allowed_actions[self.agents[self.current_agent_index]] = actions['gov'].astype(bool)\n",
    "            self.current_agent_index += 1\n",
    "        else:\n",
    "            # All agents have acted\n",
    "            assert len(actions) == len(self.agents)\n",
    "\n",
    "            # Execute transition\n",
    "            self.state = np.array([actions[agent] for agent in self.agents])\n",
    "\n",
    "            state_reward = 1 if np.all(self.state == self.state[0]) else 0\n",
    "            degree_of_restriction = np.mean([1 - (np.sum(allowed_actions) / self.NUMBER_OF_ACTIONS) for allowed_actions in self.allowed_actions.values()])\n",
    "            restriction_reward = -self.ALPHA * degree_of_restriction\n",
    "            gov_reward = state_reward + restriction_reward\n",
    "\n",
    "            self.current_step += 1\n",
    "            self.current_agent_index = 0\n",
    "            self.allowed_actions = { } # Could be removed for speed-up\n",
    "\n",
    "            # Governance needs to decide on allowed actions for first agent on the list (if episode is not over)\n",
    "            self.is_reward_step = True\n",
    "            return { 'gov': { 'state': self.state, 'obs': self.get_observation(self.agents[self.current_agent_index]) } }, \\\n",
    "                   { 'gov': gov_reward }, \\\n",
    "                   { '__all__': self.current_step >= self.NUMBER_OF_STEPS_PER_EPISODE }, \\\n",
    "                   { 'gov': (state_reward, restriction_reward, degree_of_restriction) }\n",
    "\n",
    "        # Governance needs to decide on allowed actions for next agent on the list\n",
    "        if self.current_agent_index < len(self.agents):\n",
    "            return { 'gov': { 'state': self.state, 'obs': self.get_observation(self.agents[self.current_agent_index]) } }, \\\n",
    "                   { 'gov': 0 }, \\\n",
    "                   { '__all__': False }, \\\n",
    "                   { 'gov': { } }\n",
    "\n",
    "        # Agents need to act\n",
    "        else:\n",
    "            agent_observations = { agent: self.state[self.agent_observations[agent]] for agent in self.agents }\n",
    "            agent_rewards = { agent: 1 if np.all(obs == obs[0]) else 0 for agent, obs in agent_observations.items() }\n",
    "\n",
    "            return { agent: { 'obs': self.get_observation(agent), 'allowed_actions': self.allowed_actions[agent] } for agent in self.agents }, \\\n",
    "                   { agent: agent_rewards[agent] for agent in self.agents }, \\\n",
    "                   { '__all__': False }, \\\n",
    "                   { }\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.current_agent_index = 0\n",
    "\n",
    "        self.state = np.random.randint(0, self.NUMBER_OF_ACTIONS, (len(self.agents), ))\n",
    "\n",
    "        self.allowed_actions = { }\n",
    "\n",
    "        return { 'gov': { 'state': self.state, 'obs': self.get_observation(self.agents[self.current_agent_index]) } }\n",
    "\n",
    "    def get_observation(self, agent: str):\n",
    "        return self.state[self.agent_observations[agent]]\n",
    "\n",
    "# Experiment without governance\n",
    "def run_experiment_without_governance(config):\n",
    "    NUMBER_OF_AGENTS = config['NUMBER_OF_AGENTS']\n",
    "    NUMBER_OF_ACTIONS = config['NUMBER_OF_ACTIONS']\n",
    "    NUMBER_OF_STEPS_PER_EPISODE = config['NUMBER_OF_STEPS_PER_EPISODE']\n",
    "    ALPHA = config['ALPHA']\n",
    "    ENV = config['ENV']\n",
    "    NUMBER_OF_TIMESTEPS = config['NUMBER_OF_TIMESTEPS']\n",
    "    NUMBER_OF_SAMPLES = config['NUMBER_OF_SAMPLES']\n",
    "    NAME = config['NAME']\n",
    "\n",
    "    gov_obs_space = Dict({ 'state': MultiDiscrete([NUMBER_OF_ACTIONS] * NUMBER_OF_AGENTS),\n",
    "                           'obs': Box(0, NUMBER_OF_ACTIONS - 1, shape=(3,))})\n",
    "    gov_action_space = MultiDiscrete([2] * NUMBER_OF_ACTIONS)\n",
    "\n",
    "    def policy_mapping_fn(agent_id, episode, **kwargs):\n",
    "        # TODO: Why is agent0 sometimes called?\n",
    "        if 'agent' in agent_id:\n",
    "            print(f'Invalid agent_id ({agent_id})!')\n",
    "\n",
    "        return agent_id if 'agent' not in agent_id else agent_id[5:]\n",
    "\n",
    "    run_config = {\n",
    "        'env': ENV,\n",
    "        'env_config': {\n",
    "          'NUMBER_OF_STEPS_PER_EPISODE': NUMBER_OF_STEPS_PER_EPISODE,\n",
    "          'NUMBER_OF_AGENTS': NUMBER_OF_AGENTS,\n",
    "          'NUMBER_OF_ACTIONS': NUMBER_OF_ACTIONS,\n",
    "          'ALPHA': ALPHA\n",
    "        },\n",
    "        'multiagent': {\n",
    "            'policies': {\n",
    "                **{str(i): (None, None, None, { }) for i in range(NUMBER_OF_AGENTS)},\n",
    "                'gov': (PassiveGovernancePolicy, gov_obs_space, gov_action_space, { })\n",
    "            },\n",
    "            'policy_mapping_fn': policy_mapping_fn,\n",
    "            'policies_to_train': [str(i) for i in range(NUMBER_OF_AGENTS)]\n",
    "        },\n",
    "        'callbacks': CustomMetricsLogger\n",
    "    }\n",
    "\n",
    "    return tune.run('PPO', verbose=1, config=run_config, stop={'timesteps_total': NUMBER_OF_TIMESTEPS},\n",
    "             num_samples=NUMBER_OF_SAMPLES, checkpoint_at_end=True,\n",
    "             name=NAME, local_dir=log_dir)\n",
    "\n",
    "# Experiment with governance\n",
    "def run_experiment_with_governance(config):\n",
    "    NUMBER_OF_AGENTS = config['NUMBER_OF_AGENTS']\n",
    "    NUMBER_OF_ACTIONS = config['NUMBER_OF_ACTIONS']\n",
    "    NUMBER_OF_STEPS_PER_EPISODE = config['NUMBER_OF_STEPS_PER_EPISODE']\n",
    "    ALPHA = config['ALPHA']\n",
    "    ENV = config['ENV']\n",
    "    NUMBER_OF_TIMESTEPS = config['NUMBER_OF_TIMESTEPS']\n",
    "    NUMBER_OF_SAMPLES = config['NUMBER_OF_SAMPLES']\n",
    "    NAME = config['NAME']\n",
    "\n",
    "    gov_obs_space = Dict({ 'state': MultiDiscrete([NUMBER_OF_ACTIONS] * NUMBER_OF_AGENTS),\n",
    "                           'obs': Box(0, NUMBER_OF_ACTIONS - 1, shape=(3,))})\n",
    "    gov_action_space = MultiDiscrete([2] * NUMBER_OF_ACTIONS)\n",
    "\n",
    "    def policy_mapping_fn(agent_id, episode, **kwargs):\n",
    "        # TODO: Why is agent0 sometimes called?\n",
    "        if 'agent' in agent_id:\n",
    "            print(f'Invalid agent_id ({agent_id})!')\n",
    "\n",
    "        return agent_id if 'agent' not in agent_id else agent_id[5:]\n",
    "\n",
    "    run_config = {\n",
    "        'env': ENV,\n",
    "        'env_config': {\n",
    "          'NUMBER_OF_STEPS_PER_EPISODE': NUMBER_OF_STEPS_PER_EPISODE,\n",
    "          'NUMBER_OF_AGENTS': NUMBER_OF_AGENTS,\n",
    "          'NUMBER_OF_ACTIONS': NUMBER_OF_ACTIONS,\n",
    "          'ALPHA': ALPHA\n",
    "        },\n",
    "        'multiagent': {\n",
    "            'policies': {\n",
    "                **{str(i): (None, None, None, { 'model': {'custom_model': ParametricAgentModel }, 'framework': 'tf' }) for i in range(NUMBER_OF_AGENTS)},\n",
    "                'gov': (None, gov_obs_space, gov_action_space, { })\n",
    "            },\n",
    "            'policy_mapping_fn': policy_mapping_fn,\n",
    "            'policies_to_train': [str(i) for i in range(NUMBER_OF_AGENTS)] + ['gov']\n",
    "        },\n",
    "        'callbacks': CustomMetricsLogger\n",
    "    }\n",
    "\n",
    "    return tune.run('PPO', verbose=1, config=run_config, stop={'timesteps_total': NUMBER_OF_TIMESTEPS},\n",
    "             num_samples=NUMBER_OF_SAMPLES, checkpoint_at_end=True,\n",
    "             name=NAME, local_dir=log_dir)\n",
    "\n",
    "# Chart creation\n",
    "def create_aamas_charts(ids):\n",
    "    outer_path = f'{log_dir}/aamas2022'\n",
    "\n",
    "    configurations = list(set(configuration for configuration, scenario in ids.keys()))\n",
    "    scenarios = list(set(scenario for configuration, scenario in ids.keys()))\n",
    "    kpis = ['governance_reward', 'degree_of_restriction']\n",
    "\n",
    "    metrics = {\n",
    "        'governance_reward': 'ray/tune/custom_metrics/episode_state_reward/gov_mean',\n",
    "        'degree_of_restriction': 'ray/tune/custom_metrics/episode_degree_of_restriction/gov_mean'\n",
    "    }\n",
    "\n",
    "    experiment_folders = { key: glob.glob(f'{outer_path}/*{id}*/') for key, id in ids.items() }\n",
    "    event_accumulators = { key: [EventAccumulator(f) for f in folders] for key, folders in experiment_folders.items() }\n",
    "\n",
    "    current, total = 1, sum(len(ea) for ea in event_accumulators.values())\n",
    "    for key, experiment in event_accumulators.items():\n",
    "        for ea in experiment:\n",
    "            print(f'\\rLoading EventAccumulator {current}/{total}...', end='')\n",
    "            ea.Reload()\n",
    "            current += 1\n",
    "\n",
    "    raw_data = { (configuration, scenario, kpi): [list(zip(*ea.Scalars(metrics[kpi]))) for ea in experiment] for kpi in kpis for (configuration, scenario), experiment in event_accumulators.items() }\n",
    "    processed_data = { key: { 'x': np.array(experiment[0][1]), 'y': [np.array(sample[2]) for sample in experiment] } for key, experiment in raw_data.items() }\n",
    "    final_data = { key: { 'x': experiment['x'], 'y': experiment['y'], 'mean': np.mean(experiment['y'], axis=0) } for key, experiment in processed_data.items() }\n",
    "\n",
    "    print(f'Finished!')\n",
    "\n",
    "    save_path = f'{log_dir}/aamas2022/charts'\n",
    "    plt.style.use({'figure.facecolor':'white'})\n",
    "\n",
    "    scenario_names = {\n",
    "        'umas': 'UMAS',\n",
    "        'cc': 'CC',\n",
    "        'gmas': 'GMAS'\n",
    "    }\n",
    "\n",
    "    colors = {\n",
    "        'umas': 'blue',\n",
    "        'cc': 'red',\n",
    "        'gmas': 'green'\n",
    "    }\n",
    "\n",
    "    for i, configuration in enumerate(configurations):\n",
    "        for j, kpi in enumerate(kpis):\n",
    "            for scenario in scenarios:\n",
    "                x, ys, mean = itemgetter('x', 'y', 'mean')(final_data[(configuration, scenario, kpi)])\n",
    "                color = colors[scenario]\n",
    "                for y in ys:\n",
    "                    plt.plot(x, y, color=color, alpha=0.4, linewidth=0.5)\n",
    "\n",
    "                plt.plot(x, mean, color=color, label=scenario_names[scenario])\n",
    "\n",
    "            plt.ticklabel_format(axis='x', useMathText=True)\n",
    "            plt.xlabel('$t$')\n",
    "            plt.legend()\n",
    "\n",
    "            plt.savefig(f'{save_path}/{configuration}_{kpi}.png', format='png', bbox_inches='tight')\n",
    "\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Run\n"
    }
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    # Values for original experiments:\n",
    "    # Tiny: 'NUMBER_OF_AGENTS': 5, 'NUMBER_OF_ACTIONS': 3\n",
    "    # Small: 'NUMBER_OF_AGENTS': 10, 'NUMBER_OF_ACTIONS': 5\n",
    "    # Medium: 'NUMBER_OF_AGENTS': 15, 'NUMBER_OF_ACTIONS': 7\n",
    "    # Large: 'NUMBER_OF_AGENTS': 20, 'NUMBER_OF_ACTIONS': 10\n",
    "\n",
    "    # All other values stay the same (to see the general functionality and save time, \n",
    "    # you can reduce the number of steps and/or number of samples)\n",
    "\n",
    "    ###########################################################\n",
    "    # CUSTOMIZE CONFIGURATION\n",
    "    config = {\n",
    "        'NUMBER_OF_AGENTS': 10,\n",
    "        'NUMBER_OF_ACTIONS': 5,\n",
    "        'NUMBER_OF_STEPS_PER_EPISODE': 100,\n",
    "        'ALPHA': 0.0,\n",
    "        'NUMBER_OF_TIMESTEPS': 12_000,\n",
    "        'NUMBER_OF_SAMPLES': 3,\n",
    "        'NAME': 'aamas2022'\n",
    "    }\n",
    "    ###########################################################\n",
    "\n",
    "    # UMAS\n",
    "    config['ENV'] = GMAS_Environment\n",
    "    run_experiment_without_governance(config)\n",
    "\n",
    "    # CC\n",
    "    config['ENV'] = CC_Environment\n",
    "    run_experiment_with_governance(config)\n",
    "\n",
    "    # GMAS\n",
    "    config['ENV'] = GMAS_Environment\n",
    "    run_experiment_with_governance(config)\n",
    "\n",
    "run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Visualize\n"
    }
   },
   "outputs": [],
   "source": [
    "def visualize():\n",
    "    ###########################################################\n",
    "    # CUSTOMIZE IDENTIFIERS\n",
    "    ids = {\n",
    "        ('tiny', 'umas'): '<id>',\n",
    "        ('tiny', 'cc'): '<id>',\n",
    "        ('tiny', 'gmas'): '<id>',\n",
    "        ('small', 'umas'): '<id>',\n",
    "        ('small', 'cc'): '<id>',\n",
    "        ('small', 'gmas'): '<id>',\n",
    "        ('medium', 'umas'): '<id>',\n",
    "        ('medium', 'cc'): '<id>',\n",
    "        ('medium', 'gmas'): '<id>',\n",
    "        ('large', 'umas'): '<id>',\n",
    "        ('large', 'cc'): '<id>',\n",
    "        ('large', 'gmas'): '<id>'\n",
    "    }\n",
    "    ###########################################################\n",
    "\n",
    "    create_aamas_charts(ids)\n",
    "\n",
    "visualize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
