{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Finding optimal restrictions via action elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the Smart Home experiment presented in Chapter 5 of the thesis. \n",
    "\n",
    "In the experiment, agents act on an environment consisting of binary attributes: At every step, they each choose one (or no) attribute which is then toggled. It is the task of the governance to restrict the action spaces such that the environmental state is kept within a given set of _valid states_. \n",
    "\n",
    "We show, for a number of runs with different numbers of agents, the governance utility with and without restrictions, as well as the utility improvement and the degree of restriction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "from tqdm import trange\n",
    "\n",
    "from src.agent import DeterministicAgent\n",
    "from src.env import BinaryAttributeEnvironment\n",
    "from src.governance import PassiveGovernance, EUMASGovernance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three structural units in this experiment: The entire _experiment_, a _scenario_ with multiple runs and a fixed number of agents, and a _simulation_ which represents a single run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(agents, environment, governance, number_of_steps):\n",
    "    environment.reset()\n",
    "    governance.reset()\n",
    "    for agent in agents:\n",
    "        agent.reset()\n",
    "\n",
    "    result = []\n",
    "    for step in range(number_of_steps):\n",
    "        fundamental_actions = [agent.actions for agent in agents]\n",
    "        allowed_actions = governance.restrict_actions(environment.state, fundamental_actions)\n",
    "        chosen_actions = [agent.act(environment.state, actions) for agent, actions in zip(agents, allowed_actions)]\n",
    "\n",
    "        old_state = environment.state\n",
    "        environment.move(chosen_actions)\n",
    "        new_state = environment.state\n",
    "\n",
    "        governance.learn(old_state, chosen_actions, new_state)\n",
    "\n",
    "        old_cost = governance.cost(old_state)\n",
    "        new_cost = governance.cost(new_state)\n",
    "\n",
    "        result.append([step, old_state, old_cost, fundamental_actions, allowed_actions, chosen_actions, new_state, new_cost])\n",
    "\n",
    "    return pd.DataFrame(result, columns=['step', 'old_state', 'old_cost', 'fundamental_actions', 'allowed_actions', 'chosen_actions', 'new_state', 'new_cost'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scenario(number_of_simulations, number_of_agents, number_of_variables, number_of_steps, seed=42):\n",
    "    # Initialize random number generator\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    cost_table = np.array([1.0 if (s[0] and s[2]) else 0.0 for s in itertools.product([True, False], repeat=number_of_variables)])\n",
    "    cost_threshold = 1.5 * ((number_of_variables + 1) ** (-number_of_agents))\n",
    "\n",
    "    ungoverned_simulations, governed_simulations = [], []\n",
    "    for episode in trange(number_of_simulations):\n",
    "        initial_state = rng.choice([True, False], size=(number_of_variables,))\n",
    "    \n",
    "        agents = [DeterministicAgent(np.arange(-1, number_of_variables), rng.integers(-1, number_of_variables, size=2 ** number_of_variables)) for _ in range(number_of_agents)]\n",
    "        environment = BinaryAttributeEnvironment(initial_state)\n",
    "        passive_governance = PassiveGovernance(number_of_agents, number_of_variables, cost_table)\n",
    "        active_governance = EUMASGovernance(number_of_agents, number_of_variables, cost_table, cost_threshold)\n",
    "\n",
    "        ungoverned_simulations.append(run_simulation(agents, environment, passive_governance, number_of_steps))\n",
    "        governed_simulations.append(run_simulation(agents, environment, active_governance, number_of_steps))\n",
    "    \n",
    "    return ungoverned_simulations, governed_simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "# Initialize random number generator\n",
    "rng = np.random.default_rng(seed)\n",
    "\n",
    "# Set parameters for all scenarios to be run\n",
    "parameters = [(10, 2, 5, 100), (10, 3, 5, 100), (10, 5, 5, 100)]\n",
    "\n",
    "# Run all scenarios and collect results\n",
    "governance_utility, improvement, degree_of_restriction = {}, {}, {}\n",
    "for number_of_simulations, number_of_agents, number_of_variables, number_of_steps in parameters:\n",
    "    print(f'Running {number_of_simulations} simulations with {number_of_agents} agents and {number_of_variables} variables for {number_of_steps} steps...')\n",
    "    ungoverned_simulations, governed_simulations = run_scenario(number_of_simulations, number_of_agents, number_of_variables, number_of_steps, seed=rng.integers(1_000_000))\n",
    "\n",
    "    ungoverned_utility = -pd.DataFrame([simulation.old_cost for simulation in ungoverned_simulations]).T.mean(axis=1).expanding().mean()\n",
    "    governed_utility = -pd.DataFrame([simulation.old_cost for simulation in governed_simulations]).T.mean(axis=1).expanding().mean()\n",
    "\n",
    "    governance_utility[f'{number_of_agents} agents (unrestricted)'] = ungoverned_utility\n",
    "    governance_utility[f'{number_of_agents} agents (restricted)'] = governed_utility\n",
    "\n",
    "    improvement[f'{number_of_agents} agents (restricted)'] = (ungoverned_utility - governed_utility) / ungoverned_utility\n",
    "\n",
    "    degree_of_restriction[f'{number_of_agents} agents (restricted)'] = pd.DataFrame([1 - simulation.allowed_actions.apply(np.concatenate).apply(len) / simulation.fundamental_actions.apply(np.concatenate).apply(len) for simulation in governed_simulations]).T.mean(axis=1)\n",
    "\n",
    "governance_utility = pd.DataFrame(governance_utility)\n",
    "improvement = pd.DataFrame(improvement)\n",
    "degree_of_restriction = pd.DataFrame(degree_of_restriction)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers_of_agents = [2, 3, 5]\n",
    "colors = {2: '#1f77b4', 3: '#ff7f0e', 5: '#2ca02c'}\n",
    "line_styles = {'unrestricted': 'dashed', 'restricted': 'solid'}\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 9), sharex=True)\n",
    "\n",
    "for number_of_agents in numbers_of_agents:\n",
    "    ax1.plot(governance_utility[f'{number_of_agents} agents (unrestricted)'], color=colors[number_of_agents], label='Unrestricted', linestyle=line_styles['unrestricted'])\n",
    "    ax1.plot(governance_utility[f'{number_of_agents} agents (restricted)'], color=colors[number_of_agents], label='Restricted', linestyle=line_styles['restricted'])\n",
    "\n",
    "    ax2.plot(improvement[f'{number_of_agents} agents (restricted)'], color=colors[number_of_agents], label=f'{number_of_agents} agents')\n",
    "\n",
    "    ax3.plot(degree_of_restriction[f'{number_of_agents} agents (restricted)'], color=colors[number_of_agents], label=f'{number_of_agents} agents')\n",
    "\n",
    "lgd = fig.legend(labels=[f'{number_of_agents} agents ({type})' for number_of_agents in numbers_of_agents for type in ['unrestricted', 'restricted']], \n",
    "    loc='outside lower center', \n",
    "    ncol=3,\n",
    "    bbox_to_anchor=(0.5, -0.02))\n",
    "\n",
    "ax1.set_ylabel('Governance utility')\n",
    "\n",
    "ax2.set_ylabel(r'Improvement')\n",
    "ax2.xaxis.get_major_locator().set_params(integer=True)\n",
    "ax2.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "\n",
    "ax3.set_xlabel(r'Time step $t$')\n",
    "ax3.set_ylabel('Degree of restriction')\n",
    "ax3.yaxis.set_major_formatter(mtick.PercentFormatter(1.0, 0))\n",
    "\n",
    "fig.align_labels()\n",
    "fig.savefig(f'./results/graph.pdf', dpi=300, bbox_extra_artists=(lgd,), bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
